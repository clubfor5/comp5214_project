{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,sgd\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "#train_files,test_files = get_data_files()\n",
    "#print(train_files)\n",
    "def train_val_split(train_AP_features,train_labels,fp_ratio):\n",
    "    #generate len(train_AP_features) of floats in between 0 and 1\n",
    "    train_val_split = np.random.rand(len(train_AP_features))\n",
    "    #convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "    train_val_split = train_val_split < fp_ratio #should contain ~70% percent true\n",
    "    # We will then split our given training set into training + validation \n",
    "    train_X = train_AP_features[train_val_split]\n",
    "    train_y = train_labels[train_val_split]\n",
    "    val_X = train_AP_features[~train_val_split]\n",
    "    val_y = train_labels[~train_val_split]\n",
    "    return train_X,train_y, val_X, val_y\n",
    "\n",
    "def normalization(data):\n",
    "    minVals = data.min(0)\n",
    "    maxVals = data.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normData = (data - minVals)/ranges\n",
    "    return normData,ranges,minVals\n",
    "\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv(file_name,header = 0)\n",
    "    #print(df.head(2))\n",
    "    AP_strengths = df.loc[:,'WAP001':'WAP520']\n",
    "    AP_strengths = AP_strengths.replace([100], [-100])\n",
    "    print(AP_strengths.head(2))\n",
    "    df_xy = df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    labels = np.asarray(df_xy)\n",
    "    AP_features = (np.asarray(AP_strengths))\n",
    "    \n",
    "    building_ids_str = df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    return AP_features, building_ids_str, building_floors_str, labels\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return math.sqrt((y_true[0]-y_pred[0])**2+(y_true[1]-y_pred[1])**2)\n",
    "\n",
    "\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n",
    "def save_to_log(file_name,preds_pos):\n",
    "    write_file = open(file_name,'w')\n",
    "    for pos in preds_pos:\n",
    "        line = str(pos[0])+','+str(pos[1])+'\\n'\n",
    "        write_file.write(line)\n",
    "    return \n",
    "def load_log(file_name):\n",
    "    read_file = open(file_name,'r')\n",
    "    lines = read_file.readlines()\n",
    "    pred_pos = []\n",
    "    for line in lines:\n",
    "        pos = line.split(',')\n",
    "        x = float(pos[0])\n",
    "        y = float(pos[1])\n",
    "        pred_pos.append([x,y])\n",
    "    return pred_pos\n",
    "\n",
    "def cdf(error):\n",
    "    count = len(error)\n",
    "    cdf_y = [i/count for i in range(count)]\n",
    "    error_sorted = sorted(error)\n",
    "    plt.xlim(0,50)\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(error_sorted, cdf_y)\n",
    "    plt.show()\n",
    "    return cdf_y,error_sorted\n",
    "\n",
    "def error_analysis(pred_y,true_y):\n",
    "    error =np.sqrt((pred_y[:,0]-true_y[:,0])**2+(pred_y[:,1]-true_y[:,1])**2)\n",
    "    rms_error = rms(error)\n",
    "    print('rms_error:', rms_error)\n",
    "    mean_error = sum(error)/len(error)\n",
    "    print('mean_error:', mean_error)\n",
    "    print(\"generating cdf:\")\n",
    "    cdf_y,error_sorted = cdf(error)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0    -100    -100    -100    -100    -100    -100    -100    -100    -100   \n",
      "1    -100    -100    -100    -100    -100    -100    -100    -100    -100   \n",
      "\n",
      "   WAP010  ...  WAP511  WAP512  WAP513  WAP514  WAP515  WAP516  WAP517  \\\n",
      "0    -100  ...    -100    -100    -100    -100    -100    -100    -100   \n",
      "1    -100  ...    -100    -100    -100    -100    -100    -100    -100   \n",
      "\n",
      "   WAP518  WAP519  WAP520  \n",
      "0    -100    -100    -100  \n",
      "1    -100    -100    -100  \n",
      "\n",
      "[2 rows x 520 columns]\n"
     ]
    }
   ],
   "source": [
    "AP_features, building_ids_str, building_floors_str, labels = load_data('trainingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input: \n",
    "def mlp_regression():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(512, input_dim=input_size, activation='relu', bias=True))\n",
    "    model1.add(Dense(512, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(32, activation='relu', bias=True))\n",
    "    model1.add(Dense(2, activation='sigmoid', bias=True))\n",
    "    #model1.compile(optimizer='sgd', loss=[mean_squared_error_index],metrics=[mean_squared_error_index])\n",
    "    return model1    \n",
    "\n",
    "def mlp(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    #x = L.Reshape((-1, 1))(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dense(128,activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model\n",
    "def lstm(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    #x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    x = L.Reshape((-1, 1))(x)\n",
    "    print(x.shape)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.LSTM(128, dropout=0.1, recurrent_dropout=0, return_sequences=True, activation='sigmoid')(x)\n",
    "    x = L.LSTM(64, dropout=0.1, return_sequences=False, activation='sigmoid')(x)\n",
    "    #x = L.Dense(128,activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    #x = L.Reshape((-1, 1))(x)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    #x = L.LSTM(16, dropout=0, return_sequences=False, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7541.2643  4.864921e+06      2           1      106   \n",
      "1     100  ...     100 -7536.6212  4.864934e+06      2           1      106   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 2       2       23  1371713733  \n",
      "1                 2       2       23  1371713691  \n",
      "\n",
      "[2 rows x 529 columns]\n",
      "floor id (19937, 1)\n",
      "building id: (19937,)\n",
      "[-97 -94 -94 -94 -90 -90 -88 -88 -87 -86 -86 -84 -83 -83 -83 -80]\n",
      "[154  35 141 155  90  89 102 190 103 150 191 172 171   7 149 247]\n",
      "[[-150 -150 -150 ... -150 -150 -150]\n",
      " [-150 -150 -150 ... -150 -150 -150]\n",
      " [-150 -150 -150 ... -150 -150 -150]\n",
      " ...\n",
      " [-150 -150 -150 ... -150 -150 -150]\n",
      " [-150 -150 -150 ... -150 -150 -150]\n",
      " [-150 -150 -150 ... -150 -150 -150]]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 16, 40)       20800       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 40)       160         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16)           64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 640)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 640)          10880       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1280)         0           flatten[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1280)         5120        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1280)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          327936      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256)          1024        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 1)         13          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1)            0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1)            4           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 65)           0           dense_3[0][0]                    \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           1056        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "xy (Dense)                      (None, 2)            34          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 408,243\n",
      "Trainable params: 405,057\n",
      "Non-trainable params: 3,186\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 16, 40)       20800       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 40)       160         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16)           64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 640)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 640)          10880       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1280)         0           flatten[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1280)         5120        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1280)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          327936      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256)          1024        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 1)         13          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1)            0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1)            4           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 65)           0           dense_3[0][0]                    \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           1056        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "xy (Dense)                      (None, 2)            34          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 408,243\n",
      "Trainable params: 405,057\n",
      "Non-trainable params: 3,186\n",
      "__________________________________________________________________________________________________\n",
      "[  -7691.3384     4864745.74501597] [390.51940991 270.94278403]\n",
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520    LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7515.916799  4.864890e+06      1           1        0   \n",
      "1     100  ...     100 -7383.867221  4.864840e+06      4           2        0   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 0       0        0  1380872703  \n",
      "1                 0       0       13  1381155054  \n",
      "\n",
      "[2 rows x 529 columns]\n",
      "test floor id (1111, 1)\n",
      "test building id: (1111, 1)\n",
      "[-150 -150 -150 -150 -150 -150 -150 -150 -150 -150 -150 -150 -150 -150\n",
      " -150  -91    1] (1111, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'one hot encode the dummy_labels.\\nthis is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \\nas the column names\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#import tensorflow_addons as tfa\n",
    "#from tensorflow_addons.layers import WeightNormalization\n",
    "#from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "path_train = \"trainingData.csv\"\n",
    "path_validation = \"validationData.csv\"\n",
    "train_df = pd.read_csv(path_train,header = 0)\n",
    "print(train_df.head(2))\n",
    "\n",
    "train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "#train_AP_features= np.array(train_AP_strengths.replace([100], [-100]))\n",
    "building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "floor_enc = LabelEncoder()\n",
    "floor_enc.fit(building_floors_str)\n",
    "floor_id = floor_enc.transform(building_floors_str)\n",
    "floor_id = floor_id.reshape(-1,1)\n",
    "print(\"floor id\",(floor_id.shape))\n",
    "building_enc = LabelEncoder()\n",
    "building_enc.fit(building_ids_str)\n",
    "building_id = building_enc.transform(building_ids_str)\n",
    "train_building_id = building_id.reshape(-1,1)\n",
    "print(\"building id:\",(building_id.shape))\n",
    "\n",
    "train_AP_features = np.array(train_AP_strengths.replace([100],[-150]))\n",
    "##### 1) RSSI_FLOOR 2) SSID 3) BUILDING_ID\n",
    "train_id = np.argsort(train_AP_features)[:,504:520]\n",
    "train_rssi = np.sort(train_AP_features)[:,504:520]\n",
    "print(train_rssi[10])\n",
    "print(train_id[10])\n",
    "print(train_AP_features[train_id[10]])\n",
    "#train_rssi_floor = np.hstack((train_rssi,floor_id))\n",
    "#print(train_rssi_floor[0], train_rssi_floor.shape)\n",
    "#print(train_id[0],train_rssi[0],train_AP_features[0])\n",
    "\n",
    "input_data = [train_id, train_rssi, train_building_id]\n",
    "model = mlp(input_data)\n",
    "model.summary()\n",
    "\n",
    "train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "train_labels = np.asarray(train_df_LL)\n",
    "train_y,ranges,bias =  normalization(train_labels)\n",
    "print(bias,ranges)\n",
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "print(test_df.head(2))\n",
    "test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "#test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "test_floor_enc = LabelEncoder()\n",
    "test_floor_enc.fit(building_floors_str)\n",
    "test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "test_floor_id = test_floor_id.reshape(-1,1)\n",
    "print(\"test floor id\",(test_floor_id.shape))\n",
    "test_building_enc = LabelEncoder()\n",
    "test_building_enc.fit(test_building_ids_str)\n",
    "test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "test_building_id = test_building_id.reshape(-1,1)\n",
    "print(\"test building id:\",(test_building_id.shape))\n",
    "test_AP_features = np.array(test_AP_strengths.replace([100],[-150]))\n",
    "test_id = np.argsort(test_AP_features)[:,504:520]\n",
    "test_rssi = np.sort(test_AP_features)[:,504:520]\n",
    "\n",
    "test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "\n",
    "test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "test_y = np.asarray(test_df_LL)\n",
    "#model.fit(input_data,train_y,nb_epoch=200,batch_size=128,callbacks=[\n",
    "       #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n",
    "#,EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)])\n",
    "#test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "#test_pred_y = test_pred_y * ranges + bias \n",
    "#error_analysis(test_y, test_pred_y)\n",
    "\"\"\"one hot encode the dummy_labels.\n",
    "this is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \n",
    "as the column names\n",
    "\"\"\"\n",
    "#train_X,train_y, val_X, val_y = train_val_split(train_AP_features,train_labels)\n",
    "\n",
    "#Turn the given validation set into a testing set\n",
    "#test_df = pd.read_csv(path_validation,header = 0)\n",
    "#test_AP_features = (np.asarray(test_df.loc[:,'WAP001':'WAP520']))/200+0.5\n",
    "#test_labels = np.asarray(test_df[\"BUILDINGID\"].map(str) + test_df[\"FLOOR\"].map(str))\n",
    "#test_labels = np.asarray(pd.get_dummies(test_labels))\n",
    "#input_size = 520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(xs, y,ratio):\n",
    "    new_xs = []\n",
    "    new_y = []\n",
    "    train_val_split = np.random.rand(len(y))\n",
    "    train_val_split = train_val_split < ratio #should contain ~70% percent true\n",
    "    for x in xs:\n",
    "        x = x[train_val_split]\n",
    "        new_xs.append(x)\n",
    "    y = y[train_val_split]\n",
    "    new_y.append(y)\n",
    "    return new_xs, new_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 0.1\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 16, 40)       20800       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 40)       160         embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16)           64          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 640)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 640)          10880       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1280)         0           flatten_2[0][0]                  \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1280)         5120        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1280)         0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          327936      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          32896       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 1)         13          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           8256        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 1)            4           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 65)           0           dense_8[0][0]                    \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           1056        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "xy (Dense)                      (None, 2)            34          dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 408,243\n",
      "Trainable params: 405,057\n",
      "Non-trainable params: 3,186\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fc1963b7120b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart_train_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpart_train_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rssi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_building_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mranges\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "for i in range(1,11):\n",
    "    ratio = i * 0.1\n",
    "    part_train_x, part_train_y = train_val_split(input_data, train_y, ratio) \n",
    "    print(\"ratio: \"+ str(i)) \n",
    "    model = mlp(part_train_x)\n",
    "    #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    model.fit(part_train_x,part_train_y,nb_epoch=300,batch_size=128,verbose = 0)\n",
    "    test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "    test_pred_y = test_pred_y * ranges + bias \n",
    "    error_analysis(test_y, test_pred_y)\n",
    "    fileName = 'mlp_dropout_layer_result_' + str(ratio)+ '.txt'\n",
    "    print(\"writing to file name: \", fileName)\n",
    "    file = open(fileName,'wb')\n",
    "    pickle.dump(test_pred_y,file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    \n",
    "    #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])s\n",
    "    model.fit(input_data,train_y,nb_epoch=10,batch_size=128,verbose = 1)\n",
    "    test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "    test_pred_y = test_pred_y * ranges + bias \n",
    "    error_analysis(test_y, test_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "print(test_df.head(2))\n",
    "test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "#test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "test_floor_enc = LabelEncoder()\n",
    "test_floor_enc.fit(building_floors_str)\n",
    "test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "test_floor_id = test_floor_id.reshape(-1,1)\n",
    "print(\"test floor id\",(test_floor_id.shape))\n",
    "test_building_enc = LabelEncoder()\n",
    "test_building_enc.fit(test_building_ids_str)\n",
    "test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "test_building_id = test_building_id.reshape(-1,1)\n",
    "print(\"test building id:\",(test_building_id.shape))\n",
    "test_AP_features = np.array(test_AP_strengths.replace([100],[-100]))\n",
    "test_id = np.argsort(test_AP_features)[:,500:520]\n",
    "test_rssi = np.sort(test_AP_features)[:,500:520]\n",
    "\n",
    "test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "\n",
    "test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "test_y = np.asarray(test_df_LL)\n",
    "test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "test_pred_y = test_pred_y * ranges + bias \n",
    "error_analysis(test_y, test_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    #print(seq_in, '->', seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length))\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "input_layer = L.Input(shape=(3,))\n",
    "input_layer1 = L.Reshape((1, -1))(input_layer)\n",
    "y = L.LSTM(32,return_sequences=False, activation='relu')(input_layer)\n",
    "output_layer_1 = L.Dense(1, name='output')(y)\n",
    "model = M.Model([input_layer], [output_layer_1])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
