{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,sgd\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "#train_files,test_files = get_data_files()\n",
    "#print(train_files)\n",
    "def train_val_split(train_AP_features,train_labels,fp_ratio):\n",
    "    #generate len(train_AP_features) of floats in between 0 and 1\n",
    "    train_val_split = np.random.rand(len(train_AP_features))\n",
    "    #convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "    train_val_split = train_val_split < fp_ratio #should contain ~70% percent true\n",
    "    # We will then split our given training set into training + validation \n",
    "    train_X = train_AP_features[train_val_split]\n",
    "    train_y = train_labels[train_val_split]\n",
    "    val_X = train_AP_features[~train_val_split]\n",
    "    val_y = train_labels[~train_val_split]\n",
    "    return train_X,train_y, val_X, val_y\n",
    "\n",
    "def normalization(data):\n",
    "    minVals = data.min(0)\n",
    "    maxVals = data.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normData = (data - minVals)/ranges\n",
    "    return normData,ranges,minVals\n",
    "\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv(file_name,header = 0)\n",
    "    #print(df.head(2))\n",
    "    AP_strengths = df.loc[:,'WAP001':'WAP520']\n",
    "    AP_strengths = AP_strengths.replace([100], [-100])\n",
    "    print(AP_strengths.head(2))\n",
    "    df_xy = df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    labels = np.asarray(df_xy)\n",
    "    AP_features = (np.asarray(AP_strengths))\n",
    "    \n",
    "    building_ids_str = df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    return AP_features, building_ids_str, building_floors_str, labels\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return math.sqrt((y_true[0]-y_pred[0])**2+(y_true[1]-y_pred[1])**2)\n",
    "\n",
    "\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n",
    "def save_to_log(file_name,preds_pos):\n",
    "    write_file = open(file_name,'w')\n",
    "    for pos in preds_pos:\n",
    "        line = str(pos[0])+','+str(pos[1])+'\\n'\n",
    "        write_file.write(line)\n",
    "    return \n",
    "def load_log(file_name):\n",
    "    read_file = open(file_name,'r')\n",
    "    lines = read_file.readlines()\n",
    "    pred_pos = []\n",
    "    for line in lines:\n",
    "        pos = line.split(',')\n",
    "        x = float(pos[0])\n",
    "        y = float(pos[1])\n",
    "        pred_pos.append([x,y])\n",
    "    return pred_pos\n",
    "\n",
    "def cdf(error):\n",
    "    count = len(error)\n",
    "    cdf_y = [i/count for i in range(count)]\n",
    "    error_sorted = sorted(error)\n",
    "    plt.xlim(0,50)\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(error_sorted, cdf_y)\n",
    "    plt.show()\n",
    "    return cdf_y,error_sorted\n",
    "\n",
    "def error_analysis(pred_y,true_y):\n",
    "    error =np.sqrt((pred_y[:,0]-true_y[:,0])**2+(pred_y[:,1]-true_y[:,1])**2)\n",
    "    rms_error = rms(error)\n",
    "    print('rms_error:', rms_error)\n",
    "    mean_error = sum(error)/len(error)\n",
    "    print('mean_error:', mean_error)\n",
    "    print(\"generating cdf:\")\n",
    "    cdf_y,error_sorted = cdf(error)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(input_data):\n",
    "    print(\"using CNN\")\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    print(\"input_site_layer\", input_site_layer.shape)\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    print(\"site_emb1\", site_emb.shape)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    print(\"site_emb2\", site_emb.shape)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    print(\"site_emb3\", site_emb.shape)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    print(\"before\",x.shape)\n",
    "    x = L.Reshape((128, 1))(x)\n",
    "    print(\"before2\",x.shape)\n",
    "    x = L.BatchNormalization()(x)   # input 128\n",
    "    x = L.Conv1D(32, 3, strides=1, dilation_rate=1, activation='relu')(x)   # input 128, output 126\n",
    "    print(\"CNN1\",x.shape)\n",
    "    y = x\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(64, 5, strides=2, dilation_rate=1, activation='relu')(x)   # input 126, output (126-5+0)/2+1 = 61\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(128, 7, strides=2, dilation_rate=1, activation='relu')(x)  # input 61, output (61-7+0)/2+1 = 28\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(64, 9, strides=1, dilation_rate=1, activation='relu')(x)  # input 23, output (28-9+0)/1+1 = 20\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(32, 5, strides=1, dilation_rate=1, activation='relu')(x)   # input 20, output (20-5+0)/1+1 = 16\n",
    "    print(\"CNN_5 \", x.shape)\n",
    "    x = L.Concatenate(axis=1)([x, y])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    print(\"CNN_res \", x.shape)\n",
    "    x = L.Conv1D(1, 1, strides=1, dilation_rate=1, activation='relu')(x)    # gloabl average pooling\n",
    "    x = L.BatchNormalization()(x)  \n",
    "    print(\"after cnn\", x.shape)\n",
    "    x = L.Flatten()(x)\n",
    "    print(\"after flatten\", x.shape)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(32, activation='relu')(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def cnn_lstm(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    x = L.Reshape((128, 1))(x)\n",
    "    # x = L.Reshape((-1, 1))(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    \n",
    "    x = L.Conv1D(32, 3, strides=1, dilation_rate=1, activation='relu')(x)   # input 128, output 126\n",
    "    y = x\n",
    "    print(\"CNN1\",x.shape)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(64, 5, strides=2, dilation_rate=1, activation='relu')(x)   # input 126, output (126-5+0)/2+1 = 61\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(128, 7, strides=2, dilation_rate=1, activation='relu')(x)  # input 61, output (61-7+0)/2+1 = 28\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(64, 9, strides=1, dilation_rate=1, activation='relu')(x)  # input 23, output (28-9+0)/1+1 = 20\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(32, 5, strides=1, dilation_rate=1, activation='relu')(x)   # input 20, output (20-5+0)/1+1 = 16\n",
    "    x = L.BatchNormalization()(x)\n",
    "    # x = L.Concatenate(axis=1)([x, y])\n",
    "    # x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Conv1D(1, 1, strides=1, dilation_rate=1, activation='relu')(x)    # gloabl average pooling\n",
    "    print(\"after conv1D\", x.shape)\n",
    "    x = L.BatchNormalization()(x) \n",
    "    x = L.LSTM(128, dropout=0, return_sequences=True, activation='sigmoid')(x)\n",
    "    x = L.LSTM(16, dropout=0, return_sequences=False, activation='sigmoid')(x)\n",
    "    print(\"after LSTM \", x.shape)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb]) \n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy', activation='sigmoid')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "def mlp(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    #x = L.Reshape((-1, 1))(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dense(128,activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model\n",
    "def lstm(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    #x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    x = L.Reshape((-1, 1))(x)\n",
    "    print(x.shape)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.LSTM(128, dropout=0.1, recurrent_dropout=0, return_sequences=True, activation='sigmoid')(x)\n",
    "    x = L.LSTM(64, dropout=0.1, return_sequences=False, activation='sigmoid')(x)\n",
    "    #x = L.Dense(128,activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    #x = L.Reshape((-1, 1))(x)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    #x = L.LSTM(16, dropout=0, return_sequences=False, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def altered_rssi_analysis():\n",
    "    from sklearn import neighbors\n",
    "    path_train = \"trainingData.csv\"\n",
    "    path_validation = \"validationData.csv\"\n",
    "    train_df = pd.read_csv(path_train,header = 0)\n",
    "    train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "    building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    floor_enc = LabelEncoder()\n",
    "    floor_enc.fit(building_floors_str)\n",
    "    floor_id = floor_enc.transform(building_floors_str)\n",
    "    floor_id = floor_id.reshape(-1,1)\n",
    "    building_enc = LabelEncoder()\n",
    "    building_enc.fit(building_ids_str)\n",
    "    building_id = building_enc.transform(building_ids_str)\n",
    "    train_building_id = building_id.reshape(-1,1)\n",
    "    train_AP_features = np.array(train_AP_strengths.replace([100],[-150]))\n",
    "    train_id = np.argsort(train_AP_features)[:,504:520]\n",
    "    train_rssi = np.sort(train_AP_features)[:,504:520]\n",
    "    print(train_rssi[10])\n",
    "    print(train_id[10])\n",
    "    print(train_AP_features[train_id[10]])\n",
    "    train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    train_labels = np.asarray(train_df_LL)\n",
    "    train_y,ranges,bias =  normalization(train_labels)\n",
    "    print(bias,ranges)\n",
    "    test_df = pd.read_csv(path_validation,header = 0)\n",
    "    print(test_df.head(2))\n",
    "    test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "    #test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "    test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    #print(id_label)\n",
    "    test_floor_enc = LabelEncoder()\n",
    "    test_floor_enc.fit(building_floors_str)\n",
    "    test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "    test_floor_id = test_floor_id.reshape(-1,1)\n",
    "    print(\"test floor id\",(test_floor_id.shape))\n",
    "    test_building_enc = LabelEncoder()\n",
    "    test_building_enc.fit(test_building_ids_str)\n",
    "    test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "    test_building_id = test_building_id.reshape(-1,1)\n",
    "    print(\"test building id:\",(test_building_id.shape))\n",
    "    test_AP_features = np.array(test_AP_strengths.replace([100],[-150]))\n",
    "    test_id = np.argsort(test_AP_features)[:,504:520]\n",
    "    test_rssi = np.sort(test_AP_features)[:,504:520]\n",
    "    test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "    print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "    test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    test_y = np.asarray(test_df_LL)\n",
    "    for i in range(1,6):\n",
    "        ratio = i * 2\n",
    "        train_rssi =  train_rssi + np.random.normal(0, 2*i, None)\n",
    "        input_data = [train_id, train_rssi, train_building_id]\n",
    "\n",
    "        model = cnn_lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_lstm_layer_altered_' + str(2*i)+ '_db.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        \n",
    "        model = lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'lstm_layer_altered_' + str(2*i)+ '_db.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        \n",
    "        # CNN\n",
    "        model = cnn(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_layer_altered_' + str(i*2)+ '_db.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        # MLP\n",
    "\n",
    "        model = mlp(input_data)\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'mlp_layer_altered_' + str(ratio)+ '_db.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "        #LSTM\n",
    "\n",
    "\n",
    "#altered_rssi_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_rssi_analysis():\n",
    "    from sklearn import neighbors\n",
    "    path_train = \"trainingData.csv\"\n",
    "    path_validation = \"validationData.csv\"\n",
    "    train_df = pd.read_csv(path_train,header = 0)\n",
    "    train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "    building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    floor_enc = LabelEncoder()\n",
    "    floor_enc.fit(building_floors_str)\n",
    "    floor_id = floor_enc.transform(building_floors_str)\n",
    "    floor_id = floor_id.reshape(-1,1)\n",
    "    building_enc = LabelEncoder()\n",
    "    building_enc.fit(building_ids_str)\n",
    "    building_id = building_enc.transform(building_ids_str)\n",
    "    train_building_id = building_id.reshape(-1,1)\n",
    "    train_AP_features = np.array(train_AP_strengths.replace([100],[-150]))\n",
    "    \n",
    "    \n",
    "    train_id = np.argsort(train_AP_features)[:,504:520]\n",
    "    train_rssi = np.sort(train_AP_features)[:,504:520]\n",
    "    print(train_rssi[10])\n",
    "    print(train_id[10])\n",
    "    print(train_AP_features[train_id[10]])\n",
    "    train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    train_labels = np.asarray(train_df_LL)\n",
    "    train_y,ranges,bias =  normalization(train_labels)\n",
    "    print(bias,ranges)\n",
    "    test_df = pd.read_csv(path_validation,header = 0)\n",
    "    print(test_df.head(2))\n",
    "    test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "    #test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "    test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    #print(id_label)\n",
    "    test_floor_enc = LabelEncoder()\n",
    "    test_floor_enc.fit(building_floors_str)\n",
    "    test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "    test_floor_id = test_floor_id.reshape(-1,1)\n",
    "    print(\"test floor id\",(test_floor_id.shape))\n",
    "    test_building_enc = LabelEncoder()\n",
    "    test_building_enc.fit(test_building_ids_str)\n",
    "    test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "    test_building_id = test_building_id.reshape(-1,1)\n",
    "    print(\"test building id:\",(test_building_id.shape))\n",
    "    test_AP_features = np.array(test_AP_strengths.replace([100],[-150]))\n",
    "    test_id = np.argsort(test_AP_features)[:,504:520]\n",
    "    test_rssi = np.sort(test_AP_features)[:,504:520]\n",
    "    test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "    print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "    test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    test_y = np.asarray(test_df_LL)\n",
    "    for i in [9]:\n",
    "        arr = np.random.random(size=train_rssi.shape)\n",
    "        mask = arr < i*0.1\n",
    "        print(mask)\n",
    "        train_rssi[mask] = -150 # 把标记为True的值记为0\n",
    "        print(train_rssi)\n",
    "        input_data = [train_id, train_rssi, train_building_id]\n",
    "\n",
    "\n",
    "        #LSTM\n",
    "        model = cnn_lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_lstm_layer_altered_' + str(i*0.1)+ '_ratio.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        model = lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'lstm_layer_altered_' + str(i*0.1)+ '_ratio.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        \n",
    "        # CNN\n",
    "        model = cnn(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_layer_altered_' + str(i*0.1)+ '__ratio.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        # MLP\n",
    "\n",
    "\n",
    "#masked_rssi_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 25, 35, 52, 53, 60, 80, 108, 113, 119, 132, 140, 158, 159, 168, 183, 203, 205, 215, 227, 261, 273, 281, 297, 300, 314, 328, 329, 345, 349, 369, 370, 376, 377, 385, 398, 407, 419, 422, 434, 451, 459, 471, 491, 500, 506, 513, 516, 517, 518]\n",
      "(19937, 1)\n",
      "(1111, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ScopedTFGraph.__del__ at 0x7fb143f7ddd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/c_api_util.py\", line 52, in __del__\n",
      "    c_api.TF_DeleteGraph(self.graph)\n",
      "AttributeError: 'ScopedTFGraph' object has no attribute 'graph'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 40)        20800       input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 1, 40)        160         embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 1)            4           input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 40)           0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 640)          1280        batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 680)          0           flatten_10[0][0]                 \n",
      "                                                                 dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 680)          2720        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 680)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 256)          174336      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 256)          0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 256)          1024        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 128)          32896       batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 1)         13          input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 128)          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 1)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 64)           8256        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 1)            4           flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 65)           0           dense_28[0][0]                   \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 16)           1056        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "xy (Dense)                      (None, 2)            34          dense_29[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 242,583\n",
      "Trainable params: 240,627\n",
      "Non-trainable params: 1,956\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 19937 samples\n",
      "Epoch 1/100\n",
      "19937/19937 [==============================] - 3s 129us/sample - loss: 0.0628 - mse: 0.0628\n",
      "Epoch 2/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0169 - mse: 0.0169\n",
      "Epoch 3/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0117 - mse: 0.0117\n",
      "Epoch 4/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0099 - mse: 0.0099\n",
      "Epoch 5/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0086 - mse: 0.0086\n",
      "Epoch 6/100\n",
      "19937/19937 [==============================] - 2s 75us/sample - loss: 0.0078 - mse: 0.0078\n",
      "Epoch 7/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0073 - mse: 0.0073\n",
      "Epoch 8/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 9/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0068 - mse: 0.0068\n",
      "Epoch 10/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0064 - mse: 0.0064\n",
      "Epoch 11/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0063 - mse: 0.0063\n",
      "Epoch 12/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0060 - mse: 0.0060\n",
      "Epoch 13/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0061 - mse: 0.0061\n",
      "Epoch 14/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0059 - mse: 0.0059\n",
      "Epoch 15/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0060 - mse: 0.0060\n",
      "Epoch 16/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0057 - mse: 0.0057\n",
      "Epoch 17/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0058 - mse: 0.0058\n",
      "Epoch 18/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0055 - mse: 0.0055\n",
      "Epoch 19/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0055 - mse: 0.0055\n",
      "Epoch 20/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0054 - mse: 0.0054\n",
      "Epoch 21/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0053 - mse: 0.0053\n",
      "Epoch 22/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0053 - mse: 0.0053\n",
      "Epoch 23/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0053 - mse: 0.0053\n",
      "Epoch 24/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0052 - mse: 0.0052\n",
      "Epoch 25/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0051 - mse: 0.0051\n",
      "Epoch 26/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0052 - mse: 0.0052\n",
      "Epoch 27/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0053 - mse: 0.0053\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0051 - mse: 0.0051\n",
      "Epoch 29/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0050 - mse: 0.0050\n",
      "Epoch 30/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0050 - mse: 0.0050\n",
      "Epoch 31/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0051 - mse: 0.0051\n",
      "Epoch 32/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0050 - mse: 0.0050\n",
      "Epoch 33/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0050 - mse: 0.0050\n",
      "Epoch 34/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0050 - mse: 0.0050\n",
      "Epoch 35/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0050 - mse: 0.0050\n",
      "Epoch 36/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0050 - mse: 0.0050\n",
      "Epoch 37/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 38/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 39/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 40/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 41/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 42/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 43/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 44/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 45/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 46/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 47/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 48/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 49/100\n",
      "19937/19937 [==============================] - 2s 79us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 50/100\n",
      "19937/19937 [==============================] - 2s 79us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 51/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 52/100\n",
      "19937/19937 [==============================] - 2s 79us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 53/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 54/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 55/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 56/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 57/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 58/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 59/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 60/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 61/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 62/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 63/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 64/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 65/100\n",
      "19937/19937 [==============================] - 2s 79us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 66/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 67/100\n",
      "19937/19937 [==============================] - 2s 79us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 68/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 69/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 70/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 71/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 72/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 73/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 74/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.00471s -\n",
      "Epoch 75/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 76/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 77/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 78/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 79/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 80/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 81/100\n",
      "19937/19937 [==============================] - 2s 79us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 82/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 83/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 84/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 85/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 86/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 87/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 88/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 89/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 90/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0045 - mse: 0.0045\n",
      "Epoch 91/100\n",
      "19937/19937 [==============================] - 2s 79us/sample - loss: 0.0045 - mse: 0.0045\n",
      "Epoch 92/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 93/100\n",
      "19937/19937 [==============================] - 2s 79us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 94/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0045 - mse: 0.0045\n",
      "Epoch 95/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 96/100\n",
      "19937/19937 [==============================] - 2s 77us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 97/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 98/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 99/100\n",
      "19937/19937 [==============================] - 2s 76us/sample - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 100/100\n",
      "19937/19937 [==============================] - 2s 78us/sample - loss: 0.0046 - mse: 0.0046\n",
      "rms_error: 40.9550782234582\n",
      "mean_error: 31.762398569048088\n",
      "generating cdf:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file name:  mlp_layer_ap_ratio0.1.txt\n",
      "CNN1 (None, 126, 32)\n",
      "after conv1D (None, 16, 1)\n",
      "after LSTM  (None, 16)\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2325\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'batch_normalization_49/cond' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2329\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'batch_normalization_49/cond' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6dab5fa36b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m#model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rssi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_building_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mranges\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[0;31m# function we recompile the metrics based on the updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m   \u001b[0;31m# sample_weight_mode value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;31m# Prepare validation data. Hold references to the iterator and the input list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m    553\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2029\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1972\u001b[0m           \u001b[0;31m# Training updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m           updates = self.optimizer.get_updates(\n\u001b[0;32m-> 1974\u001b[0;31m               params=self._collected_trainable_weights, loss=self.total_loss)\n\u001b[0m\u001b[1;32m   1975\u001b[0m       \u001b[0;31m# Unconditional updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_updates_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     self._assert_valid_dtypes([\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scope_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 677\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    678\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    347\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 677\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    678\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_IfGrad\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;31m# Get the if operator (this logic handles the case where op is a MockOp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0mif_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   \u001b[0mtrue_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_func_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mif_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m   \u001b[0;31m# Note: op.graph != ops.get_default_graph() when we are computing the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# of a nested cond.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_get_func_graphs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"If\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     return (_get_func_graph_for_branch(op.get_attr(\"then_branch\")),\n\u001b[0;32m--> 269\u001b[0;31m             _get_func_graph_for_branch(op.get_attr(\"else_branch\")))\n\u001b[0m\u001b[1;32m    270\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Case\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     return [_get_func_graph_for_branch(branch_fn)\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_get_func_graph_for_branch\u001b[0;34m(name_attr_list)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m       func_graph = function_def_to_graph.function_def_to_graph(\n\u001b[0;32m--> 258\u001b[0;31m           fdef, input_shapes)\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexternal_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_t\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0mcustom_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexternal_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, input_shapes)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Add all function nodes to the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Initialize fields specific to FuncGraph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    429\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# Create _DefinedFunctions for any imported functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/c_api_util.py\u001b[0m in \u001b[0;36mtf_buffer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def select_random_columns(ratio):\n",
    "    select_columns = []\n",
    "    for i in range(520):\n",
    "        if np.random.random() < ratio:\n",
    "            select_columns.append(i)\n",
    "    return select_columns\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sklearn import neighbors\n",
    "    ratios = [0.3,0.7,0.5,0.3,0.1]\n",
    "    for ratio in ratios:\n",
    "        columns = select_random_columns(ratio)\n",
    "        print(columns)\n",
    "        path_train = \"trainingData.csv\"\n",
    "        path_validation = \"validationData.csv\"\n",
    "        train_df = pd.read_csv(path_train,header = 0)\n",
    "        train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "        building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "        building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "        floor_enc = LabelEncoder()\n",
    "        floor_enc.fit(building_floors_str)\n",
    "        floor_id = floor_enc.transform(building_floors_str)\n",
    "        floor_id = floor_id.reshape(-1,1)\n",
    "        building_enc = LabelEncoder()\n",
    "        building_enc.fit(building_ids_str)\n",
    "        building_id = building_enc.transform(building_ids_str)\n",
    "        train_building_id = building_id.reshape(-1,1)\n",
    "        train_AP_features = np.array(train_AP_strengths.replace([100],[-150]))   \n",
    "        train_AP_features = train_AP_features[:,columns]\n",
    "        \n",
    "        dimensions = train_AP_features.shape[1]\n",
    "        train_id = np.argsort(train_AP_features)[:,dimensions-int(16*ratio):dimensions]\n",
    "        train_rssi = np.sort(train_AP_features)[:,dimensions-int(16*ratio):dimensions]\n",
    "        print(train_rssi.shape)\n",
    "        #print(train_rssi[10])\n",
    "        #print(train_id[10])\n",
    "        #print(train_AP_features[train_id[10]])\n",
    "        train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "        train_labels = np.asarray(train_df_LL)\n",
    "        train_y,ranges,bias =  normalization(train_labels)\n",
    "        #print(bias,ranges)\n",
    "        test_df = pd.read_csv(path_validation,header = 0)\n",
    "        #print(test_df.head(2))\n",
    "        test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "        #test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "        test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "        test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "        #print(id_label)\n",
    "        test_floor_enc = LabelEncoder()\n",
    "        test_floor_enc.fit(building_floors_str)\n",
    "        test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "        test_floor_id = test_floor_id.reshape(-1,1)\n",
    "        #print(\"test floor id\",(test_floor_id.shape))\n",
    "        test_building_enc = LabelEncoder()\n",
    "        test_building_enc.fit(test_building_ids_str)\n",
    "        test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "        test_building_id = test_building_id.reshape(-1,1)\n",
    "        #print(\"test building id:\",(test_building_id.shape))\n",
    "        test_AP_features = np.array(test_AP_strengths.replace([100],[-150]))\n",
    "        test_AP_features = test_AP_features[:,columns]\n",
    "        test_id = np.argsort(test_AP_features)[:,dimensions-int(16*ratio):dimensions]\n",
    "        test_rssi = np.sort(test_AP_features)[:,dimensions-int(16*ratio):dimensions]\n",
    "        print(test_rssi.shape)\n",
    "        test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "        #print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "        test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "        test_y = np.asarray(test_df_LL)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        input_data = [train_id, train_rssi, train_building_id]\n",
    "        #LSTM\n",
    "        \n",
    "        \n",
    "        model = mlp(input_data)\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'mlp_layer_ap_ratio' + str(ratio)+ '.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        model = cnn_lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_lstm_layer_ap_ratio' + str(i*0.1)+ '.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        model = lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'lstm_layer_ap_ratio' + str(i*0.1)+ '.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        \n",
    "        # CNN\n",
    "        model = cnn(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_layer_ap_ratio' + str(i*0.1)+ '.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
