{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"trainingData.csv\"\n",
    "path_validation = \"validationData.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_files():\n",
    "    train_files = []\n",
    "    test_files = []\n",
    "    import os\n",
    "    dir = sourceFolder\n",
    "    dbtype_list = os.listdir(dir)\n",
    "    for dbtype in dbtype_list:\n",
    "        if dbtype.find(\"train\")!= -1:\n",
    "            train_files.append(sourceFolder+dbtype)\n",
    "        elif dbtype.find(\"test\")!= -1:\n",
    "            test_files.append(sourceFolder+dbtype)\n",
    "    train_files =sorted(train_files,key=str.lower)\n",
    "    test_files =sorted(test_files,key=str.lower)\n",
    "    return train_files, test_files\n",
    "\n",
    "#train_files,test_files = get_data_files()\n",
    "#print(train_files)\n",
    "#print(test_files)\n",
    "def load_train_test_data(file):\n",
    "    train_df = pd.read_csv(file, header=0)\n",
    "    fp = train_df.iloc[:,1:-3]\n",
    "    loc = train_df.iloc[:,[-3,-2]]\n",
    "    ap_num = fp.columns.size\n",
    "    return fp,loc,ap_num\n",
    "def load_test_data(file):\n",
    "    test_df = pd.read_csv(file,header = 0)\n",
    "    fps = test_df.iloc[:,1:-1]\n",
    "    fp_size = fps.columns.size\n",
    "    fps = (np.asarray(fps))/200 + 0.5\n",
    "    where_are_nan = np.isnan(fps)\n",
    "    fps[where_are_nan] = 1\n",
    "    site_path_ts = test_df.iloc[:,-1]\n",
    "    return fps,fp_size,site_path_ts\n",
    "def train_val_split(train_AP_features,train_labels):\n",
    "    #generate len(train_AP_features) of floats in between 0 and 1\n",
    "    train_val_split = np.random.rand(len(train_AP_features))\n",
    "    #convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "    train_val_split = train_val_split < 1 #should contain ~70% percent true\n",
    "    # We will then split our given training set into training + validation \n",
    "    train_X = train_AP_features[train_val_split]\n",
    "    train_y = train_labels[train_val_split]\n",
    "    val_X = train_AP_features[~train_val_split]\n",
    "    val_y = train_labels[~train_val_split]\n",
    "    return train_X,train_y, val_X, val_y\n",
    "def normalization(data):\n",
    "    minVals = data.min(0)\n",
    "    maxVals = data.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normData = (data - minVals)/ranges\n",
    "    return normData,ranges,minVals\n",
    "\n",
    "def load_train_data(file_name):\n",
    "    fps,loc,ap_num = load_train_test_data(file_name)\n",
    "    fps = (np.asarray(fps))/200 + 0.5\n",
    "    where_are_nan = np.isnan(fps)\n",
    "    fps[where_are_nan] = 1\n",
    "    labels = np.asarray(loc) #labels is an array of shape 19937 x 13. (there are 13 types of labels)\n",
    "    labels,ranges,minVals =  normalization(labels)\n",
    "    train_X,train_y, val_X, val_y =  train_val_split(fps, labels)\n",
    "    return train_X,train_y, val_X, val_y,ranges,ap_num,minVals\n",
    "\n",
    "def dec2bin(x,digits):\n",
    "    digits_total = digits \n",
    "    bins = []\n",
    "    if x>=1:\n",
    "        x = 0.99999999\n",
    "    x -= int(x)\n",
    "    \n",
    "    while x > 0 and digits > 0:\n",
    "        x *= 2\n",
    "        bins.append(1 if x>=1. else 0)\n",
    "        x -= int(x)\n",
    "        digits -= 1\n",
    "    while len(bins) < digits_total:\n",
    "        bins.append(0)\n",
    "    return bins\n",
    "def get_bin_digits(num):\n",
    "    digits = 0\n",
    "    while num >= 1:\n",
    "        num/= 2\n",
    "        digits+=1\n",
    "    return digits\n",
    "def get_code(point,digit):\n",
    "    x = dec2bin(point[0],digit[0])\n",
    "    y = dec2bin(point[1],digit[1])\n",
    "    code_word = x+y\n",
    "    return code_word\n",
    "\n",
    "def bin2dec(bins):\n",
    "    dec = 0\n",
    "    frac = 1/2\n",
    "    for bin in bins:\n",
    "        dec += (bin) * frac\n",
    "        frac *= 1/2\n",
    "    return dec\n",
    "def code_to_point(codes,digit):\n",
    "    assert len(codes) == digit[0] + digit[1]\n",
    "    x = bin2dec(codes[0:digit[0]])\n",
    "    y = bin2dec(codes[digit[0]:])\n",
    "    return (x,y)\n",
    "def get_bin_digits(num):\n",
    "    digits = 0\n",
    "    while num >= 1:\n",
    "        num/= 2\n",
    "        digits+=1\n",
    "    return digits\n",
    "def get_code(point,digit):\n",
    "    x = dec2bin(point[0],digit[0])\n",
    "    y = dec2bin(point[1],digit[1])\n",
    "    code_word = x+y\n",
    "    return code_word\n",
    "\n",
    "def bin2dec(bins):\n",
    "    dec = 0\n",
    "    frac = 1/2\n",
    "    for bin in bins:\n",
    "        dec += (bin) * frac\n",
    "        frac *= 1/2\n",
    "    return dec\n",
    "def code_to_point(codes,digit):\n",
    "    assert len(codes) == digit[0] + digit[1]\n",
    "    x = bin2dec(codes[0:digit[0]])\n",
    "    y = bin2dec(codes[digit[0]:])\n",
    "    return (x,y)\n",
    "from keras.optimizers import Adam,sgd\n",
    "def hirarchical_structure_loss(y_true,y_pred):\n",
    "    xent_loss = losses.binary_crossentropy(y_true, y_pred)\n",
    "    #print(xent_loss)\n",
    "    print(y_true.shape)\n",
    "    weights = [1 for i in range(sum(digit))]\n",
    "    print(weights)\n",
    "    weights= K.constant(weights)\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, weights)\n",
    "    return xent_loss\n",
    "\n",
    "def weighted_hirarchical_structure_loss(digit,beta=0):\n",
    "    def weighted_hirarchical_structure(y_true,y_pred):\n",
    "        xent_loss = losses.binary_crossentropy(y_true, y_pred)\n",
    "        #print(xent_loss)\n",
    "        weights = [1-beta*(i%(sum(digit)/2)) for i in range(sum(digit))]\n",
    "        bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        return bce(y_true, y_pred, sample_weight=weights)\n",
    "    return weighted_hirarchical_structure\n",
    "def mean_squared_error_index(ranges = [1,1]):\n",
    "    def mean_squared_error_ind(y_true, y_pred):\n",
    "        return K.mean(K.square(y_pred*ranges - y_true*ranges), axis=-1)\n",
    "    return mean_squared_error_ind\n",
    "\n",
    "def mean_squared_error_index_out(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "def cross_loss(appended_ranges,beta=0):\n",
    "    def cross_structure(y_true,y_pred):\n",
    "        y_true_part1 = y_true[:,0:18]\n",
    "        y_true_part2 = y_true[:,18:]\n",
    "        y_pred_part1 = y_pred[:,0:18]\n",
    "        y_pred_part2 = y_pred[:,18:]\n",
    "        loss1 = losses.binary_crossentropy(y_true_part1, y_pred_part1)\n",
    "        loss2 = losses.mse(y_true_part2,y_pred_part2)\n",
    "        #print(xent_loss)\n",
    "        #weights = [1 for i in range(18)]\n",
    "        #weights.append(0)\n",
    "        #weights.append(0)\n",
    "        #print(weights)\n",
    "        #weight_2 = [0 for i in range(18)]\n",
    "        #weight_2.append(0)\n",
    "        #weight_2.append(0)\n",
    "        #bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        #loss1 = bce(y_true, y_pred, sample_weight=weights)\n",
    "        #loss2 = mean_squared_error_index_out(y_true* weight_2*appended_ranges/100, y_pred* weight_2*appended_ranges/100)\n",
    "        return loss1+10*loss2\n",
    "    return cross_structure\n",
    "    \n",
    "def cross_hierarchical_soft_max(appended_ranges):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=input_size, activation='relu', bias=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu', bias=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu', bias=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu', bias=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='tanh', bias=True))\n",
    "    model.add(Dense((sum(digit)+2), activation='sigmoid', bias=True))\n",
    "    #print(sum(digit))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=[cross_loss(appended_ranges)],metrics=[cross_loss(appended_ranges)])\n",
    "    return model\n",
    "    \n",
    "def hirarchical_softmax(digit,beta=0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_size, activation='relu', bias=True))\n",
    "    model.add(Dense(512, activation='relu', bias=True))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', bias=True))\n",
    "    model.add(Dense(128, activation='relu', bias=True))\n",
    "    model.add(Dense(32, activation='relu', bias=True))\n",
    "    output_num = sum(digit) + 2\n",
    "    print(output_num)\n",
    "    model.add(Dense(output_num, activation='sigmoid', bias=True))\n",
    "    #print(sum(digit))\n",
    "    model.compile(optimizer=sgd(learning_rate=0.01), loss=[weighted_hirarchical_structure_loss(digit,beta=beta)],metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def mlp_regression():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(1024, input_dim=input_size, activation='relu', bias=True))\n",
    "    model1.add(Dense(512, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(32, activation='relu', bias=True))\n",
    "    model1.add(Dense(2, activation='sigmoid', bias=True))\n",
    "    #model1.compile(optimizer='sgd', loss=[mean_squared_error_index],metrics=[mean_squared_error_index])\n",
    "    return model1    \n",
    "\n",
    "def code2pos_batch(codes,digit,ranges,bias):\n",
    "    points = [] \n",
    "    for code in codes:\n",
    "        points.append(code_to_point(pred,digit))\n",
    "    return points * ranges + bias\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return math.sqrt((y_true[0]-y_pred[0])**2+(y_true[1]-y_pred[1])**2)\n",
    "\n",
    "def cdf(data):\n",
    "    hist, bin_edges = np.histogram(data,bins=100)\n",
    "    cdf = np.cumsum(hist/sum(hist))\n",
    "    return bin_edges[1:],cdf\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7541.2643  4.864921e+06      2           1      106   \n",
      "1     100  ...     100 -7536.6212  4.864934e+06      2           1      106   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 2       2       23  1371713733  \n",
      "1                 2       2       23  1371713691  \n",
      "\n",
      "[2 rows x 529 columns]\n"
     ]
    }
   ],
   "source": [
    "#Explicitly pass header=0 to be able to replace existing names \n",
    "train_df = pd.read_csv(path_train,header = 0)\n",
    "train_df = train_df[:19930]\n",
    "print(train_df.head(2))\n",
    "train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "train_AP_features = (np.asarray(train_AP_strengths))/200 + 0.5\n",
    "#The following two objects are actually pandas.core.series.Series objects\n",
    "building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "\n",
    "res = building_ids_str + building_floors_str #element wise concatenation of BUILDINGID+FLOOR\n",
    "train_labels = np.asarray(building_ids_str + building_floors_str)\n",
    "\n",
    "#convert labels to categorical variables, dummy_labels has type 'pandas.core.frame.DataFrame'\n",
    "dummy_labels = pd.get_dummies(train_labels)\n",
    "\n",
    "\"\"\"one hot encode the dummy_labels.\n",
    "this is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \n",
    "as the column names\n",
    "\"\"\"\n",
    "train_labels = np.asarray(dummy_labels) #labels is an array of shape 19937 x 13. (there are 13 types of labels)\n",
    "train_X,train_y, val_X, val_y = train_val_split(train_AP_features,train_labels)\n",
    "\n",
    "#Turn the given validation set into a testing set\n",
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "test_AP_features = (np.asarray(test_df.loc[:,'WAP001':'WAP520']))/200+0.5\n",
    "test_labels = np.asarray(test_df[\"BUILDINGID\"].map(str) + test_df[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))\n",
    "input_size = 520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'digit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d9ef12f8554c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cross_model = cross_hierarchical_soft_max(appended_ranges)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhirarchical_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_code_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'digit' is not defined"
     ]
    }
   ],
   "source": [
    "#cross_model = cross_hierarchical_soft_max(appended_ranges)\n",
    "hi = hirarchical_softmax(digit,0)\n",
    "hi.summary()\n",
    "print(train_y)\n",
    "print(train_code_y)\n",
    "print(sum(digit))\n",
    "#print(cross_train_code_y.shape)\n",
    "#print(train_code_y)\n",
    "#train_code_y = np.reshape(19563,18)\n",
    "batch_size = 64\n",
    "#print(appended_ranges)\n",
    "hi.fit(train_X,train_code_y,nb_epoch=200,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[390.51940991 270.94278403]\n",
      "[390.51940991 270.94278403]\n",
      "19937\n",
      "19937\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example point:  [0.38429357 0.6460153 ]\n",
      "example code [0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "convert code back to pos: (0.3828125, 0.64453125)\n",
      "[[0.         1.         1.         ... 0.         0.38429357 0.6460153 ]\n",
      " [0.         1.         1.         ... 0.         0.39618312 0.69564571]\n",
      " [0.         1.         1.         ... 1.         0.44091534 0.75214103]\n",
      " ...\n",
      " [0.         1.         1.         ... 1.         0.44683285 0.52980183]\n",
      " [0.         1.         1.         ... 1.         0.39438885 0.55373567]\n",
      " [0.         1.         1.         ... 1.         0.39734926 0.56142549]]\n",
      "[[0. 1. 1. ... 0. 1. 0.]\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " [0. 1. 1. ... 1. 1. 1.]]\n",
      "check training feature's shape:  (19937, 20)\n",
      "check training feature's shape:  (19937, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:176: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, input_dim=520, activation=\"relu\", use_bias=True)`\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:178: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", use_bias=True)`\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:180: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", use_bias=True)`\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:182: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", use_bias=True)`\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:184: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"tanh\", use_bias=True)`\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:185: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, activation=\"sigmoid\", use_bias=True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1024)              533504    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 1,617,748\n",
      "Trainable params: 1,617,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "19937/19937 [==============================] - 6s 306us/step - loss: 1.0570 - cross_structure: 1.0570\n",
      "Epoch 2/1000\n",
      "19937/19937 [==============================] - 5s 273us/step - loss: 0.7524 - cross_structure: 0.7524\n",
      "Epoch 3/1000\n",
      "19937/19937 [==============================] - 6s 280us/step - loss: 0.7123 - cross_structure: 0.7123\n",
      "Epoch 4/1000\n",
      "19937/19937 [==============================] - 6s 276us/step - loss: 0.6890 - cross_structure: 0.6890\n",
      "Epoch 5/1000\n",
      "19937/19937 [==============================] - 5s 275us/step - loss: 0.6745 - cross_structure: 0.6745\n",
      "Epoch 6/1000\n",
      "19937/19937 [==============================] - 6s 278us/step - loss: 0.6663 - cross_structure: 0.6663\n",
      "Epoch 7/1000\n",
      "19937/19937 [==============================] - 6s 277us/step - loss: 0.6577 - cross_structure: 0.6577\n",
      "Epoch 8/1000\n",
      "19937/19937 [==============================] - 6s 279us/step - loss: 0.6519 - cross_structure: 0.6519\n",
      "Epoch 9/1000\n",
      "19937/19937 [==============================] - 6s 276us/step - loss: 0.6475 - cross_structure: 0.6475\n",
      "Epoch 10/1000\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.6424 - cross_structure: 0.6424\n",
      "Epoch 11/1000\n",
      "19937/19937 [==============================] - 5s 268us/step - loss: 0.6381 - cross_structure: 0.6381\n",
      "Epoch 12/1000\n",
      "19937/19937 [==============================] - 5s 262us/step - loss: 0.6336 - cross_structure: 0.6336\n",
      "Epoch 13/1000\n",
      "19937/19937 [==============================] - 5s 269us/step - loss: 0.6307 - cross_structure: 0.6307\n",
      "Epoch 14/1000\n",
      "19937/19937 [==============================] - 7s 328us/step - loss: 0.6267 - cross_structure: 0.6267\n",
      "Epoch 15/1000\n",
      "19937/19937 [==============================] - 5s 255us/step - loss: 0.6227 - cross_structure: 0.6227\n",
      "Epoch 16/1000\n",
      "19937/19937 [==============================] - 5s 264us/step - loss: 0.6203 - cross_structure: 0.6203\n",
      "Epoch 17/1000\n",
      "19937/19937 [==============================] - 5s 252us/step - loss: 0.6163 - cross_structure: 0.6163\n",
      "Epoch 18/1000\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.6135 - cross_structure: 0.6135\n",
      "Epoch 19/1000\n",
      "19937/19937 [==============================] - 5s 260us/step - loss: 0.6088 - cross_structure: 0.6088\n",
      "Epoch 20/1000\n",
      "19937/19937 [==============================] - 5s 244us/step - loss: 0.6057 - cross_structure: 0.6057\n",
      "Epoch 21/1000\n",
      "19937/19937 [==============================] - 5s 251us/step - loss: 0.6028 - cross_structure: 0.6028\n",
      "Epoch 22/1000\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.5992 - cross_structure: 0.5992\n",
      "Epoch 23/1000\n",
      "19937/19937 [==============================] - 6s 324us/step - loss: 0.5972 - cross_structure: 0.5972\n",
      "Epoch 24/1000\n",
      "19937/19937 [==============================] - 6s 288us/step - loss: 0.5938 - cross_structure: 0.5938\n",
      "Epoch 25/1000\n",
      "19937/19937 [==============================] - 5s 256us/step - loss: 0.5915 - cross_structure: 0.5915\n",
      "Epoch 26/1000\n",
      "19937/19937 [==============================] - 5s 255us/step - loss: 0.5885 - cross_structure: 0.5885\n",
      "Epoch 27/1000\n",
      "19937/19937 [==============================] - 5s 252us/step - loss: 0.5855 - cross_structure: 0.5855\n",
      "Epoch 28/1000\n",
      "19937/19937 [==============================] - 5s 250us/step - loss: 0.5836 - cross_structure: 0.5836\n",
      "Epoch 29/1000\n",
      "19937/19937 [==============================] - 5s 252us/step - loss: 0.5805 - cross_structure: 0.5805\n",
      "Epoch 30/1000\n",
      "19937/19937 [==============================] - 5s 249us/step - loss: 0.5789 - cross_structure: 0.5789\n",
      "Epoch 31/1000\n",
      "19937/19937 [==============================] - 5s 253us/step - loss: 0.5766 - cross_structure: 0.5766\n",
      "Epoch 32/1000\n",
      "19937/19937 [==============================] - 5s 249us/step - loss: 0.5736 - cross_structure: 0.5736\n",
      "Epoch 33/1000\n",
      "19937/19937 [==============================] - 5s 253us/step - loss: 0.5730 - cross_structure: 0.5730\n",
      "Epoch 34/1000\n",
      "19937/19937 [==============================] - 5s 250us/step - loss: 0.5709 - cross_structure: 0.5709\n",
      "Epoch 35/1000\n",
      "19937/19937 [==============================] - 5s 265us/step - loss: 0.5690 - cross_structure: 0.5690\n",
      "Epoch 36/1000\n",
      "19937/19937 [==============================] - 5s 262us/step - loss: 0.5672 - cross_structure: 0.5672\n",
      "Epoch 37/1000\n",
      "19937/19937 [==============================] - 5s 256us/step - loss: 0.5659 - cross_structure: 0.5659\n",
      "Epoch 38/1000\n",
      "19937/19937 [==============================] - 5s 251us/step - loss: 0.5646 - cross_structure: 0.5646\n",
      "Epoch 39/1000\n",
      "19937/19937 [==============================] - 5s 257us/step - loss: 0.5634 - cross_structure: 0.5634\n",
      "Epoch 40/1000\n",
      "19937/19937 [==============================] - 5s 257us/step - loss: 0.5612 - cross_structure: 0.5612\n",
      "Epoch 41/1000\n",
      "19937/19937 [==============================] - 6s 305us/step - loss: 0.5601 - cross_structure: 0.5601\n",
      "Epoch 42/1000\n",
      "19937/19937 [==============================] - 6s 308us/step - loss: 0.5593 - cross_structure: 0.5593\n",
      "Epoch 43/1000\n",
      "19937/19937 [==============================] - 6s 321us/step - loss: 0.5583 - cross_structure: 0.5583\n",
      "Epoch 44/1000\n",
      "19937/19937 [==============================] - 6s 294us/step - loss: 0.5572 - cross_structure: 0.5572\n",
      "Epoch 45/1000\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.5560 - cross_structure: 0.5560\n",
      "Epoch 46/1000\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.5553 - cross_structure: 0.5553\n",
      "Epoch 47/1000\n",
      "19937/19937 [==============================] - 6s 292us/step - loss: 0.5535 - cross_structure: 0.5535\n",
      "Epoch 48/1000\n",
      "19937/19937 [==============================] - 6s 311us/step - loss: 0.5537 - cross_structure: 0.5537\n",
      "Epoch 49/1000\n",
      "19937/19937 [==============================] - 5s 275us/step - loss: 0.5522 - cross_structure: 0.5522\n",
      "Epoch 50/1000\n",
      "19937/19937 [==============================] - 7s 359us/step - loss: 0.5517 - cross_structure: 0.5517\n",
      "Epoch 51/1000\n",
      "19937/19937 [==============================] - 8s 392us/step - loss: 0.5508 - cross_structure: 0.5508\n",
      "Epoch 52/1000\n",
      "19937/19937 [==============================] - 7s 345us/step - loss: 0.5492 - cross_structure: 0.5492 1s - loss: 0\n",
      "Epoch 53/1000\n",
      "19937/19937 [==============================] - 7s 352us/step - loss: 0.5487 - cross_structure: 0.5487\n",
      "Epoch 54/1000\n",
      "19937/19937 [==============================] - 7s 328us/step - loss: 0.5480 - cross_structure: 0.5480\n",
      "Epoch 55/1000\n",
      "10750/19937 [===============>..............] - ETA: 2s - loss: 0.5477 - cross_structure: 0.5477"
     ]
    }
   ],
   "source": [
    "longitude_scale = 0.00001141\n",
    "latitude_scale = 0.00000899\n",
    "def normalization(data):\n",
    "    minVals = data.min(0)\n",
    "    maxVals = data.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normData = (data - minVals)/ranges\n",
    "    return normData,ranges,minVals\n",
    "from tensorflow.keras import metrics,losses\n",
    "train_df = pd.read_csv(path_train,header = 0)\n",
    "train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "train_AP_features = (np.asarray(train_AP_strengths))/200 + 0.5\n",
    "train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "train_labels = np.asarray(train_df_LL)\n",
    "train_labels,ranges,train_bias =  normalization(train_labels)\n",
    "print(ranges)\n",
    "#convert labels to categorical variables, dummy_labels has type 'pandas.core.frame.DataFrame'\n",
    "#dummy_labels = pd.get_dummies(train_labels)\n",
    "appended_ranges = np.append([0 for i in range(18)],ranges)\n",
    "\"\"\"one hot encode the dummy_labels.\n",
    "this is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \n",
    "as the column names\n",
    "\"\"\"\n",
    "#train_labels = np.asarray(dummy_labels) #labels is an array of shape 19937 x 13. (there are 13 types of labels)\n",
    "train_X,train_y, val_X, val_y = train_val_split(train_AP_features,train_labels)\n",
    "print(ranges)\n",
    "x = train_y[:,0]*ranges[0]\n",
    "y = train_y[:,1]*ranges[1]\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "from matplotlib import pyplot as plt\n",
    "px = [y[0] for y in train_y]\n",
    "py = [y[1] for y in train_y]\n",
    "plt.scatter(px,py)\n",
    "plt.show()\n",
    "grid_size = 1\n",
    "digit_x = get_bin_digits(ranges[0]/grid_size)\n",
    "digit_y = get_bin_digits(ranges[1]/grid_size)\n",
    "digit = [digit_x, digit_y]\n",
    "example_point = train_y[0]\n",
    "print(\"example point: \",example_point)\n",
    "code = get_code(example_point,digit)\n",
    "print(\"example code\",code)\n",
    "point_back  = code_to_point(code,digit)\n",
    "print(\"convert code back to pos:\",point_back)\n",
    "train_code_y = np.empty(shape=(0, sum(digit)))\n",
    "cross_train_code_y = np.empty(shape=(0, sum(digit)+2))\n",
    "for y in train_y:\n",
    "    code = get_code(y,digit)\n",
    "    cross_code = []\n",
    "    #print(code)\n",
    "    cross_code.extend(code)\n",
    "    #print(cross_code)\n",
    "    cross_code.extend(y)\n",
    "    #print(cross_code)\n",
    "    train_code_y = np.append(train_code_y,[code],axis=0)\n",
    "    cross_train_code_y = np.append(cross_train_code_y,[cross_code],axis=0)\n",
    "#print(\"finish coding all training labels, train_code_y: \",train_code_y[0])\n",
    "print(cross_train_code_y)\n",
    "print(train_code_y)\n",
    "print(\"check training feature's shape: \", cross_train_code_y.shape)\n",
    "print(\"check training feature's shape: \", train_code_y.shape)\n",
    "#print(\"check testing feature's shape: \",test_code_y.shape)\n",
    "#hirarchical_model = hirarchical_softmax(digit,beta=0)\n",
    "batch_size = 50\n",
    "#hirarchical_model.compile(optimizer=sgd(learning_rate=0.01), loss=[hirarchical_structure_loss],metrics=['binary_accuracy'])\n",
    "#hirarchical_model.fit(train_X, train_code_y, nb_epoch=1000, batch_size=batch_size)\n",
    "cross_model = cross_hierarchical_soft_max(appended_ranges)\n",
    "cross_model.compile(optimizer=sgd(learning_rate=0.01), loss=[cross_loss(appended_ranges)],metrics=[cross_loss(appended_ranges)])\n",
    "cross_model.summary()\n",
    "cross_model.fit(train_X,cross_train_code_y,nb_epoch=1000,batch_size=batch_size)\n",
    "#print(\"check testing feature's shape: \",test_code_y.shape)\n",
    "#hirarchical_model = hirarchical_softmax(digit,0)\n",
    "#batch_size = 50\n",
    "#hirarchical_model.compile(optimizer=sgd(learning_rate=0.01), loss=[hirarchical_structure_loss],metrics=['binary_accuracy'])\n",
    "#hirarchical_model.fit(train_X, cross_train_code_y, nb_epoch=50, batch_size=64)\n",
    "#hirarchical_model.save('hiera_'+site_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hirarchical_model.compile(optimizer=sgd(learning_rate=0.01), loss=[hirarchical_structure_loss],metrics=['binary_accuracy'])\n",
    "hirarchical_model.fit(train_X, train_code_y, nb_epoch=200, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  650/19937 [..............................] - ETA: 5s - loss: 0.3280 - cross_structure: 0.3280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.3262 - cross_structure: 0.3262\n",
      "Epoch 2/500\n",
      "19937/19937 [==============================] - 6s 278us/step - loss: 0.3259 - cross_structure: 0.3259\n",
      "Epoch 3/500\n",
      "19937/19937 [==============================] - 6s 286us/step - loss: 0.3265 - cross_structure: 0.3265\n",
      "Epoch 4/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.3260 - cross_structure: 0.3260\n",
      "Epoch 5/500\n",
      "19937/19937 [==============================] - 6s 280us/step - loss: 0.3250 - cross_structure: 0.3250\n",
      "Epoch 6/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.3253 - cross_structure: 0.3253\n",
      "Epoch 7/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.3256 - cross_structure: 0.3256\n",
      "Epoch 8/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.3243 - cross_structure: 0.3243\n",
      "Epoch 9/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.3255 - cross_structure: 0.3255\n",
      "Epoch 10/500\n",
      "19937/19937 [==============================] - 6s 286us/step - loss: 0.3250 - cross_structure: 0.3250\n",
      "Epoch 11/500\n",
      "19937/19937 [==============================] - 6s 289us/step - loss: 0.3242 - cross_structure: 0.3242\n",
      "Epoch 12/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.3257 - cross_structure: 0.3257\n",
      "Epoch 13/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3244 - cross_structure: 0.3244\n",
      "Epoch 14/500\n",
      "19937/19937 [==============================] - 6s 309us/step - loss: 0.3241 - cross_structure: 0.3241\n",
      "Epoch 15/500\n",
      "19937/19937 [==============================] - 6s 308us/step - loss: 0.3235 - cross_structure: 0.3235\n",
      "Epoch 16/500\n",
      "19937/19937 [==============================] - 6s 310us/step - loss: 0.3243 - cross_structure: 0.3243\n",
      "Epoch 17/500\n",
      "19937/19937 [==============================] - 6s 310us/step - loss: 0.3245 - cross_structure: 0.3245\n",
      "Epoch 18/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3254 - cross_structure: 0.3254\n",
      "Epoch 19/500\n",
      "19937/19937 [==============================] - 6s 307us/step - loss: 0.3245 - cross_structure: 0.3245\n",
      "Epoch 20/500\n",
      "19937/19937 [==============================] - 6s 311us/step - loss: 0.3235 - cross_structure: 0.3235\n",
      "Epoch 21/500\n",
      "19937/19937 [==============================] - 6s 311us/step - loss: 0.3228 - cross_structure: 0.3228\n",
      "Epoch 22/500\n",
      "19937/19937 [==============================] - 6s 313us/step - loss: 0.3235 - cross_structure: 0.3235\n",
      "Epoch 23/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3235 - cross_structure: 0.3235\n",
      "Epoch 24/500\n",
      "19937/19937 [==============================] - 6s 308us/step - loss: 0.3236 - cross_structure: 0.3236\n",
      "Epoch 25/500\n",
      "19937/19937 [==============================] - 6s 320us/step - loss: 0.3223 - cross_structure: 0.3223\n",
      "Epoch 26/500\n",
      "19937/19937 [==============================] - 6s 313us/step - loss: 0.3231 - cross_structure: 0.3231\n",
      "Epoch 27/500\n",
      "19937/19937 [==============================] - 6s 313us/step - loss: 0.3236 - cross_structure: 0.3236\n",
      "Epoch 28/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3222 - cross_structure: 0.3222\n",
      "Epoch 29/500\n",
      "19937/19937 [==============================] - 6s 317us/step - loss: 0.3226 - cross_structure: 0.3226\n",
      "Epoch 30/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3229 - cross_structure: 0.3229\n",
      "Epoch 31/500\n",
      "19937/19937 [==============================] - 6s 313us/step - loss: 0.3220 - cross_structure: 0.3220\n",
      "Epoch 32/500\n",
      "19937/19937 [==============================] - 6s 318us/step - loss: 0.3222 - cross_structure: 0.3222\n",
      "Epoch 33/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3220 - cross_structure: 0.3220\n",
      "Epoch 34/500\n",
      "19937/19937 [==============================] - 6s 317us/step - loss: 0.3210 - cross_structure: 0.3210\n",
      "Epoch 35/500\n",
      "19937/19937 [==============================] - 6s 314us/step - loss: 0.3218 - cross_structure: 0.3218\n",
      "Epoch 36/500\n",
      "19937/19937 [==============================] - 6s 316us/step - loss: 0.3220 - cross_structure: 0.3220\n",
      "Epoch 37/500\n",
      "19937/19937 [==============================] - 6s 309us/step - loss: 0.3213 - cross_structure: 0.3213\n",
      "Epoch 38/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3212 - cross_structure: 0.3212\n",
      "Epoch 39/500\n",
      "19937/19937 [==============================] - 6s 314us/step - loss: 0.3204 - cross_structure: 0.3204\n",
      "Epoch 40/500\n",
      "19937/19937 [==============================] - 6s 310us/step - loss: 0.3225 - cross_structure: 0.3225\n",
      "Epoch 41/500\n",
      "19937/19937 [==============================] - 6s 311us/step - loss: 0.3216 - cross_structure: 0.3216\n",
      "Epoch 42/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3203 - cross_structure: 0.3203\n",
      "Epoch 43/500\n",
      "19937/19937 [==============================] - 6s 316us/step - loss: 0.3214 - cross_structure: 0.3214\n",
      "Epoch 44/500\n",
      "19937/19937 [==============================] - 6s 316us/step - loss: 0.3219 - cross_structure: 0.3219\n",
      "Epoch 45/500\n",
      "19937/19937 [==============================] - 6s 310us/step - loss: 0.3213 - cross_structure: 0.3213\n",
      "Epoch 46/500\n",
      "19937/19937 [==============================] - 6s 322us/step - loss: 0.3202 - cross_structure: 0.3202\n",
      "Epoch 47/500\n",
      "19937/19937 [==============================] - 6s 324us/step - loss: 0.3204 - cross_structure: 0.3204\n",
      "Epoch 48/500\n",
      "19937/19937 [==============================] - 6s 323us/step - loss: 0.3209 - cross_structure: 0.3209\n",
      "Epoch 49/500\n",
      "19937/19937 [==============================] - 6s 314us/step - loss: 0.3194 - cross_structure: 0.3194\n",
      "Epoch 50/500\n",
      "19937/19937 [==============================] - 6s 323us/step - loss: 0.3195 - cross_structure: 0.3195\n",
      "Epoch 51/500\n",
      "19937/19937 [==============================] - 6s 315us/step - loss: 0.3191 - cross_structure: 0.3191\n",
      "Epoch 52/500\n",
      "19937/19937 [==============================] - 6s 317us/step - loss: 0.3195 - cross_structure: 0.3195\n",
      "Epoch 53/500\n",
      "19937/19937 [==============================] - 6s 315us/step - loss: 0.3202 - cross_structure: 0.3202\n",
      "Epoch 54/500\n",
      "19937/19937 [==============================] - 6s 315us/step - loss: 0.3198 - cross_structure: 0.3198\n",
      "Epoch 55/500\n",
      "19937/19937 [==============================] - 6s 315us/step - loss: 0.3196 - cross_structure: 0.3196\n",
      "Epoch 56/500\n",
      "19937/19937 [==============================] - 6s 308us/step - loss: 0.3197 - cross_structure: 0.3197\n",
      "Epoch 57/500\n",
      "19937/19937 [==============================] - 6s 316us/step - loss: 0.3201 - cross_structure: 0.3201\n",
      "Epoch 58/500\n",
      "19937/19937 [==============================] - 6s 316us/step - loss: 0.3189 - cross_structure: 0.3189\n",
      "Epoch 59/500\n",
      "19937/19937 [==============================] - 6s 314us/step - loss: 0.3194 - cross_structure: 0.3194\n",
      "Epoch 60/500\n",
      "19937/19937 [==============================] - 6s 310us/step - loss: 0.3198 - cross_structure: 0.3198\n",
      "Epoch 61/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3201 - cross_structure: 0.3201\n",
      "Epoch 62/500\n",
      "19937/19937 [==============================] - 6s 307us/step - loss: 0.3187 - cross_structure: 0.3187\n",
      "Epoch 63/500\n",
      "19937/19937 [==============================] - 6s 313us/step - loss: 0.3173 - cross_structure: 0.3173\n",
      "Epoch 64/500\n",
      "19937/19937 [==============================] - 6s 309us/step - loss: 0.3192 - cross_structure: 0.3192\n",
      "Epoch 65/500\n",
      "19937/19937 [==============================] - 6s 309us/step - loss: 0.3176 - cross_structure: 0.3176\n",
      "Epoch 66/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.3170 - cross_structure: 0.3170\n",
      "Epoch 67/500\n",
      "19937/19937 [==============================] - 6s 313us/step - loss: 0.3172 - cross_structure: 0.3172\n",
      "Epoch 68/500\n",
      "19937/19937 [==============================] - 6s 310us/step - loss: 0.3179 - cross_structure: 0.3179\n",
      "Epoch 69/500\n",
      "19937/19937 [==============================] - 6s 315us/step - loss: 0.3165 - cross_structure: 0.3165\n",
      "Epoch 70/500\n",
      "19937/19937 [==============================] - 6s 310us/step - loss: 0.3173 - cross_structure: 0.3173\n",
      "Epoch 71/500\n",
      "19937/19937 [==============================] - 6s 311us/step - loss: 0.3177 - cross_structure: 0.3177\n",
      "Epoch 72/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3161 - cross_structure: 0.3161\n",
      "Epoch 73/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.3169 - cross_structure: 0.3169\n",
      "Epoch 74/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3166 - cross_structure: 0.3166\n",
      "Epoch 75/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.3163 - cross_structure: 0.3163\n",
      "Epoch 76/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3164 - cross_structure: 0.3164\n",
      "Epoch 77/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3182 - cross_structure: 0.3182\n",
      "Epoch 78/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3159 - cross_structure: 0.3159\n",
      "Epoch 79/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3177 - cross_structure: 0.3177\n",
      "Epoch 80/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3161 - cross_structure: 0.3161\n",
      "Epoch 81/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3159 - cross_structure: 0.3159\n",
      "Epoch 82/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3167 - cross_structure: 0.3167\n",
      "Epoch 83/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3162 - cross_structure: 0.3162\n",
      "Epoch 84/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3157 - cross_structure: 0.3157\n",
      "Epoch 85/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3157 - cross_structure: 0.3157\n",
      "Epoch 86/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3155 - cross_structure: 0.3155\n",
      "Epoch 87/500\n",
      "19937/19937 [==============================] - 6s 323us/step - loss: 0.3157 - cross_structure: 0.3157\n",
      "Epoch 88/500\n",
      "19937/19937 [==============================] - 6s 306us/step - loss: 0.3139 - cross_structure: 0.3139\n",
      "Epoch 89/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3130 - cross_structure: 0.3130\n",
      "Epoch 90/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3148 - cross_structure: 0.3148\n",
      "Epoch 91/500\n",
      "19937/19937 [==============================] - 6s 306us/step - loss: 0.3143 - cross_structure: 0.3143\n",
      "Epoch 92/500\n",
      "19937/19937 [==============================] - 6s 304us/step - loss: 0.3154 - cross_structure: 0.3154\n",
      "Epoch 93/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3143 - cross_structure: 0.3143\n",
      "Epoch 94/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3146 - cross_structure: 0.3146\n",
      "Epoch 95/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3144 - cross_structure: 0.3144\n",
      "Epoch 96/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3145 - cross_structure: 0.3145\n",
      "Epoch 97/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3140 - cross_structure: 0.3140\n",
      "Epoch 98/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3146 - cross_structure: 0.3146\n",
      "Epoch 99/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.3146 - cross_structure: 0.3146\n",
      "Epoch 100/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3135 - cross_structure: 0.3135\n",
      "Epoch 101/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3127 - cross_structure: 0.3127\n",
      "Epoch 102/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.3135 - cross_structure: 0.3135\n",
      "Epoch 103/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3132 - cross_structure: 0.3132\n",
      "Epoch 104/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3128 - cross_structure: 0.3128\n",
      "Epoch 105/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.3125 - cross_structure: 0.3125\n",
      "Epoch 106/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3126 - cross_structure: 0.3126\n",
      "Epoch 107/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.3117 - cross_structure: 0.3117\n",
      "Epoch 108/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3116 - cross_structure: 0.3116\n",
      "Epoch 109/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3122 - cross_structure: 0.3122\n",
      "Epoch 110/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3115 - cross_structure: 0.3115\n",
      "Epoch 111/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3128 - cross_structure: 0.3128\n",
      "Epoch 112/500\n",
      "19937/19937 [==============================] - 6s 304us/step - loss: 0.3136 - cross_structure: 0.3136\n",
      "Epoch 113/500\n",
      "19937/19937 [==============================] - 6s 295us/step - loss: 0.3120 - cross_structure: 0.3120\n",
      "Epoch 114/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3119 - cross_structure: 0.3119\n",
      "Epoch 115/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3132 - cross_structure: 0.3132\n",
      "Epoch 116/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3125 - cross_structure: 0.3125\n",
      "Epoch 117/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3112 - cross_structure: 0.3112\n",
      "Epoch 118/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3117 - cross_structure: 0.3116\n",
      "Epoch 119/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3113 - cross_structure: 0.3113\n",
      "Epoch 120/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3111 - cross_structure: 0.3111\n",
      "Epoch 121/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3101 - cross_structure: 0.3101\n",
      "Epoch 122/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3118 - cross_structure: 0.3118\n",
      "Epoch 123/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.3102 - cross_structure: 0.3102\n",
      "Epoch 124/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3108 - cross_structure: 0.3108\n",
      "Epoch 125/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3110 - cross_structure: 0.3110\n",
      "Epoch 126/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3103 - cross_structure: 0.3103\n",
      "Epoch 127/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3098 - cross_structure: 0.3098\n",
      "Epoch 128/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3103 - cross_structure: 0.3103\n",
      "Epoch 129/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3112 - cross_structure: 0.3112\n",
      "Epoch 130/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3096 - cross_structure: 0.3096\n",
      "Epoch 131/500\n",
      "19937/19937 [==============================] - 6s 304us/step - loss: 0.3097 - cross_structure: 0.3097\n",
      "Epoch 132/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3102 - cross_structure: 0.3102\n",
      "Epoch 133/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3099 - cross_structure: 0.3099\n",
      "Epoch 134/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3099 - cross_structure: 0.3099\n",
      "Epoch 135/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3088 - cross_structure: 0.3088\n",
      "Epoch 136/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3105 - cross_structure: 0.3105\n",
      "Epoch 137/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3099 - cross_structure: 0.3099\n",
      "Epoch 138/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3081 - cross_structure: 0.3081\n",
      "Epoch 139/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3086 - cross_structure: 0.3086\n",
      "Epoch 140/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3096 - cross_structure: 0.3096\n",
      "Epoch 141/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3103 - cross_structure: 0.3103\n",
      "Epoch 142/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3089 - cross_structure: 0.3089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3076 - cross_structure: 0.3076\n",
      "Epoch 144/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3082 - cross_structure: 0.3082\n",
      "Epoch 145/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3078 - cross_structure: 0.3078\n",
      "Epoch 146/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.3087 - cross_structure: 0.3087\n",
      "Epoch 147/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3071 - cross_structure: 0.3071\n",
      "Epoch 148/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3080 - cross_structure: 0.3080\n",
      "Epoch 149/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3067 - cross_structure: 0.3067\n",
      "Epoch 150/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3072 - cross_structure: 0.3072\n",
      "Epoch 151/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3067 - cross_structure: 0.3067\n",
      "Epoch 152/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3070 - cross_structure: 0.3070\n",
      "Epoch 153/500\n",
      "19937/19937 [==============================] - 6s 305us/step - loss: 0.3079 - cross_structure: 0.3079\n",
      "Epoch 154/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3071 - cross_structure: 0.3071\n",
      "Epoch 155/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.3072 - cross_structure: 0.3072\n",
      "Epoch 156/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3070 - cross_structure: 0.3070\n",
      "Epoch 157/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3073 - cross_structure: 0.3073\n",
      "Epoch 158/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3067 - cross_structure: 0.3067\n",
      "Epoch 159/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3065 - cross_structure: 0.3065\n",
      "Epoch 160/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3061 - cross_structure: 0.3061\n",
      "Epoch 161/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3062 - cross_structure: 0.3062\n",
      "Epoch 162/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3071 - cross_structure: 0.3071\n",
      "Epoch 163/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3064 - cross_structure: 0.3064\n",
      "Epoch 164/500\n",
      "19937/19937 [==============================] - 6s 295us/step - loss: 0.3052 - cross_structure: 0.3052\n",
      "Epoch 165/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3059 - cross_structure: 0.3059\n",
      "Epoch 166/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3060 - cross_structure: 0.3060\n",
      "Epoch 167/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3058 - cross_structure: 0.3058\n",
      "Epoch 168/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3057 - cross_structure: 0.3057\n",
      "Epoch 169/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3060 - cross_structure: 0.3060\n",
      "Epoch 170/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3038 - cross_structure: 0.3038\n",
      "Epoch 171/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3054 - cross_structure: 0.3054\n",
      "Epoch 172/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3052 - cross_structure: 0.3052\n",
      "Epoch 173/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3060 - cross_structure: 0.3060\n",
      "Epoch 174/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3048 - cross_structure: 0.3048\n",
      "Epoch 175/500\n",
      "19937/19937 [==============================] - 6s 306us/step - loss: 0.3053 - cross_structure: 0.3053\n",
      "Epoch 176/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3051 - cross_structure: 0.3051\n",
      "Epoch 177/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.3039 - cross_structure: 0.3039\n",
      "Epoch 178/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3053 - cross_structure: 0.3053\n",
      "Epoch 179/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3040 - cross_structure: 0.3040\n",
      "Epoch 180/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3040 - cross_structure: 0.3040\n",
      "Epoch 181/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3035 - cross_structure: 0.3035\n",
      "Epoch 182/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3029 - cross_structure: 0.3029\n",
      "Epoch 183/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3039 - cross_structure: 0.3039\n",
      "Epoch 184/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3032 - cross_structure: 0.3032\n",
      "Epoch 185/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3038 - cross_structure: 0.3038\n",
      "Epoch 186/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3026 - cross_structure: 0.3026\n",
      "Epoch 187/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3035 - cross_structure: 0.3035\n",
      "Epoch 188/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3030 - cross_structure: 0.3030\n",
      "Epoch 189/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3029 - cross_structure: 0.3029\n",
      "Epoch 190/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3027 - cross_structure: 0.3027\n",
      "Epoch 191/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3029 - cross_structure: 0.3029\n",
      "Epoch 192/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3021 - cross_structure: 0.3021\n",
      "Epoch 193/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3019 - cross_structure: 0.3019\n",
      "Epoch 194/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3015 - cross_structure: 0.3015\n",
      "Epoch 195/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3024 - cross_structure: 0.3024\n",
      "Epoch 196/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3015 - cross_structure: 0.3015\n",
      "Epoch 197/500\n",
      "19937/19937 [==============================] - 6s 306us/step - loss: 0.3011 - cross_structure: 0.3011\n",
      "Epoch 198/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3025 - cross_structure: 0.3025\n",
      "Epoch 199/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3028 - cross_structure: 0.3028\n",
      "Epoch 200/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3027 - cross_structure: 0.3027\n",
      "Epoch 201/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3010 - cross_structure: 0.3010\n",
      "Epoch 202/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.3012 - cross_structure: 0.3012\n",
      "Epoch 203/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3007 - cross_structure: 0.3007\n",
      "Epoch 204/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3011 - cross_structure: 0.3011\n",
      "Epoch 205/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.3012 - cross_structure: 0.3012\n",
      "Epoch 206/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2997 - cross_structure: 0.2997\n",
      "Epoch 207/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2994 - cross_structure: 0.2994\n",
      "Epoch 208/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.3008 - cross_structure: 0.3008\n",
      "Epoch 209/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.3012 - cross_structure: 0.3012\n",
      "Epoch 210/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.3006 - cross_structure: 0.3006\n",
      "Epoch 211/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2993 - cross_structure: 0.2993\n",
      "Epoch 212/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2991 - cross_structure: 0.2991\n",
      "Epoch 213/500\n",
      "19937/19937 [==============================] - 6s 295us/step - loss: 0.3006 - cross_structure: 0.3006\n",
      "Epoch 214/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.3000 - cross_structure: 0.3000\n",
      "Epoch 215/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2998 - cross_structure: 0.2998\n",
      "Epoch 216/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2999 - cross_structure: 0.2999\n",
      "Epoch 217/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.3005 - cross_structure: 0.3005\n",
      "Epoch 218/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2997 - cross_structure: 0.2997\n",
      "Epoch 219/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2995 - cross_structure: 0.2995\n",
      "Epoch 220/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2986 - cross_structure: 0.2986\n",
      "Epoch 221/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2998 - cross_structure: 0.2998\n",
      "Epoch 222/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2974 - cross_structure: 0.2974\n",
      "Epoch 223/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2976 - cross_structure: 0.2976\n",
      "Epoch 224/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2992 - cross_structure: 0.2992\n",
      "Epoch 225/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2970 - cross_structure: 0.2970\n",
      "Epoch 226/500\n",
      "19937/19937 [==============================] - 6s 295us/step - loss: 0.2984 - cross_structure: 0.2984\n",
      "Epoch 227/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2981 - cross_structure: 0.2981\n",
      "Epoch 228/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2982 - cross_structure: 0.2982\n",
      "Epoch 229/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2994 - cross_structure: 0.2994\n",
      "Epoch 230/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2981 - cross_structure: 0.2981\n",
      "Epoch 231/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2992 - cross_structure: 0.2992\n",
      "Epoch 232/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2979 - cross_structure: 0.2979\n",
      "Epoch 233/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2970 - cross_structure: 0.2970\n",
      "Epoch 234/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2979 - cross_structure: 0.2979\n",
      "Epoch 235/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2976 - cross_structure: 0.2976\n",
      "Epoch 236/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2976 - cross_structure: 0.2976\n",
      "Epoch 237/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2987 - cross_structure: 0.2987\n",
      "Epoch 238/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2972 - cross_structure: 0.2972\n",
      "Epoch 239/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2956 - cross_structure: 0.2956\n",
      "Epoch 240/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2968 - cross_structure: 0.2968\n",
      "Epoch 241/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2962 - cross_structure: 0.2962\n",
      "Epoch 242/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2967 - cross_structure: 0.2967\n",
      "Epoch 243/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2962 - cross_structure: 0.2962\n",
      "Epoch 244/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2979 - cross_structure: 0.2979\n",
      "Epoch 245/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2964 - cross_structure: 0.2964\n",
      "Epoch 246/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2957 - cross_structure: 0.2957\n",
      "Epoch 247/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2957 - cross_structure: 0.2957\n",
      "Epoch 248/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2960 - cross_structure: 0.2960\n",
      "Epoch 249/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2956 - cross_structure: 0.2956\n",
      "Epoch 250/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2957 - cross_structure: 0.2957\n",
      "Epoch 251/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2952 - cross_structure: 0.2952\n",
      "Epoch 252/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2965 - cross_structure: 0.2965\n",
      "Epoch 253/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2955 - cross_structure: 0.2955\n",
      "Epoch 254/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2945 - cross_structure: 0.2945\n",
      "Epoch 255/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2956 - cross_structure: 0.2956\n",
      "Epoch 256/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2957 - cross_structure: 0.2957\n",
      "Epoch 257/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2942 - cross_structure: 0.2942\n",
      "Epoch 258/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2946 - cross_structure: 0.2946\n",
      "Epoch 259/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2942 - cross_structure: 0.2942\n",
      "Epoch 260/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2944 - cross_structure: 0.2944\n",
      "Epoch 261/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2950 - cross_structure: 0.2950\n",
      "Epoch 262/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2933 - cross_structure: 0.2933\n",
      "Epoch 263/500\n",
      "19937/19937 [==============================] - 6s 295us/step - loss: 0.2938 - cross_structure: 0.2938\n",
      "Epoch 264/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2935 - cross_structure: 0.2935\n",
      "Epoch 265/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2931 - cross_structure: 0.2931\n",
      "Epoch 266/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2939 - cross_structure: 0.2939\n",
      "Epoch 267/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2936 - cross_structure: 0.2936\n",
      "Epoch 268/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2932 - cross_structure: 0.2932\n",
      "Epoch 269/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2928 - cross_structure: 0.2928\n",
      "Epoch 270/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.2934 - cross_structure: 0.2934\n",
      "Epoch 271/500\n",
      "19937/19937 [==============================] - 7s 326us/step - loss: 0.2930 - cross_structure: 0.2930\n",
      "Epoch 272/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2933 - cross_structure: 0.2933\n",
      "Epoch 273/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2936 - cross_structure: 0.2936\n",
      "Epoch 274/500\n",
      "19937/19937 [==============================] - 6s 311us/step - loss: 0.2922 - cross_structure: 0.2922\n",
      "Epoch 275/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2926 - cross_structure: 0.2926\n",
      "Epoch 276/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2932 - cross_structure: 0.2932\n",
      "Epoch 277/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2931 - cross_structure: 0.2931\n",
      "Epoch 278/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2930 - cross_structure: 0.2930\n",
      "Epoch 279/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2929 - cross_structure: 0.2929\n",
      "Epoch 280/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2926 - cross_structure: 0.2926\n",
      "Epoch 281/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2923 - cross_structure: 0.2923\n",
      "Epoch 282/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2922 - cross_structure: 0.2922\n",
      "Epoch 283/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2916 - cross_structure: 0.2916\n",
      "Epoch 284/500\n",
      "19937/19937 [==============================] - 6s 311us/step - loss: 0.2907 - cross_structure: 0.2907\n",
      "Epoch 285/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2913 - cross_structure: 0.2913\n",
      "Epoch 286/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2915 - cross_structure: 0.2915\n",
      "Epoch 287/500\n",
      "19937/19937 [==============================] - 6s 304us/step - loss: 0.2914 - cross_structure: 0.2914\n",
      "Epoch 288/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2911 - cross_structure: 0.2911\n",
      "Epoch 289/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2927 - cross_structure: 0.2927\n",
      "Epoch 290/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2915 - cross_structure: 0.2915\n",
      "Epoch 291/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2908 - cross_structure: 0.2908\n",
      "Epoch 292/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2908 - cross_structure: 0.2908\n",
      "Epoch 293/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2923 - cross_structure: 0.2923\n",
      "Epoch 294/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2912 - cross_structure: 0.2912\n",
      "Epoch 295/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2906 - cross_structure: 0.2906\n",
      "Epoch 296/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2901 - cross_structure: 0.2901\n",
      "Epoch 297/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2902 - cross_structure: 0.2902\n",
      "Epoch 298/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2902 - cross_structure: 0.2902\n",
      "Epoch 299/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2898 - cross_structure: 0.2898\n",
      "Epoch 300/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2911 - cross_structure: 0.2911\n",
      "Epoch 301/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2888 - cross_structure: 0.2888\n",
      "Epoch 302/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2890 - cross_structure: 0.2890\n",
      "Epoch 303/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2899 - cross_structure: 0.2899\n",
      "Epoch 304/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2898 - cross_structure: 0.2898\n",
      "Epoch 305/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2901 - cross_structure: 0.2901\n",
      "Epoch 306/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2891 - cross_structure: 0.2891\n",
      "Epoch 307/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2885 - cross_structure: 0.2885\n",
      "Epoch 308/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2882 - cross_structure: 0.2882\n",
      "Epoch 309/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2878 - cross_structure: 0.2878\n",
      "Epoch 310/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2877 - cross_structure: 0.2877\n",
      "Epoch 311/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2884 - cross_structure: 0.2884\n",
      "Epoch 312/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2900 - cross_structure: 0.2900\n",
      "Epoch 313/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2885 - cross_structure: 0.2885\n",
      "Epoch 314/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2875 - cross_structure: 0.2875\n",
      "Epoch 315/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2887 - cross_structure: 0.2887\n",
      "Epoch 316/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2875 - cross_structure: 0.2875\n",
      "Epoch 317/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2876 - cross_structure: 0.2876\n",
      "Epoch 318/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2882 - cross_structure: 0.2882\n",
      "Epoch 319/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2882 - cross_structure: 0.2882\n",
      "Epoch 320/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2890 - cross_structure: 0.2890\n",
      "Epoch 321/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2878 - cross_structure: 0.2878\n",
      "Epoch 322/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2883 - cross_structure: 0.2883\n",
      "Epoch 323/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2879 - cross_structure: 0.2879\n",
      "Epoch 324/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2866 - cross_structure: 0.2866\n",
      "Epoch 325/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2880 - cross_structure: 0.2880\n",
      "Epoch 326/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2879 - cross_structure: 0.2879\n",
      "Epoch 327/500\n",
      "19937/19937 [==============================] - 6s 309us/step - loss: 0.2867 - cross_structure: 0.2867\n",
      "Epoch 328/500\n",
      "19937/19937 [==============================] - 6s 308us/step - loss: 0.2867 - cross_structure: 0.2867\n",
      "Epoch 329/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2861 - cross_structure: 0.2861\n",
      "Epoch 330/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2870 - cross_structure: 0.2870\n",
      "Epoch 331/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2858 - cross_structure: 0.2858\n",
      "Epoch 332/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2858 - cross_structure: 0.2858\n",
      "Epoch 333/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2858 - cross_structure: 0.2858\n",
      "Epoch 334/500\n",
      "19937/19937 [==============================] - 6s 304us/step - loss: 0.2854 - cross_structure: 0.2854\n",
      "Epoch 335/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2863 - cross_structure: 0.2863\n",
      "Epoch 336/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2870 - cross_structure: 0.2870\n",
      "Epoch 337/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2860 - cross_structure: 0.2860\n",
      "Epoch 338/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2862 - cross_structure: 0.2862\n",
      "Epoch 339/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2858 - cross_structure: 0.2858\n",
      "Epoch 340/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2839 - cross_structure: 0.2839\n",
      "Epoch 341/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2846 - cross_structure: 0.2846\n",
      "Epoch 342/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2860 - cross_structure: 0.2860\n",
      "Epoch 343/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2852 - cross_structure: 0.2852\n",
      "Epoch 344/500\n",
      "19937/19937 [==============================] - 6s 313us/step - loss: 0.2851 - cross_structure: 0.2851\n",
      "Epoch 345/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2840 - cross_structure: 0.2840\n",
      "Epoch 346/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2849 - cross_structure: 0.2849\n",
      "Epoch 347/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2842 - cross_structure: 0.2842\n",
      "Epoch 348/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2848 - cross_structure: 0.2848\n",
      "Epoch 349/500\n",
      "19937/19937 [==============================] - 6s 312us/step - loss: 0.2845 - cross_structure: 0.2845\n",
      "Epoch 350/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2830 - cross_structure: 0.2830\n",
      "Epoch 351/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2847 - cross_structure: 0.2847\n",
      "Epoch 352/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2841 - cross_structure: 0.2841\n",
      "Epoch 353/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2837 - cross_structure: 0.2837\n",
      "Epoch 354/500\n",
      "19937/19937 [==============================] - 6s 304us/step - loss: 0.2831 - cross_structure: 0.2831\n",
      "Epoch 355/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2831 - cross_structure: 0.2831\n",
      "Epoch 356/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2834 - cross_structure: 0.2834\n",
      "Epoch 357/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2843 - cross_structure: 0.2843\n",
      "Epoch 358/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2821 - cross_structure: 0.2821\n",
      "Epoch 359/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2844 - cross_structure: 0.2844\n",
      "Epoch 360/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2831 - cross_structure: 0.2831\n",
      "Epoch 361/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2832 - cross_structure: 0.2832\n",
      "Epoch 362/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2826 - cross_structure: 0.2826\n",
      "Epoch 363/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2821 - cross_structure: 0.2821\n",
      "Epoch 364/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2830 - cross_structure: 0.2830\n",
      "Epoch 365/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2829 - cross_structure: 0.2829\n",
      "Epoch 366/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2834 - cross_structure: 0.2834\n",
      "Epoch 367/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2816 - cross_structure: 0.2816\n",
      "Epoch 368/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2841 - cross_structure: 0.2841\n",
      "Epoch 369/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2814 - cross_structure: 0.2814\n",
      "Epoch 370/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2821 - cross_structure: 0.2821\n",
      "Epoch 371/500\n",
      "19937/19937 [==============================] - 6s 309us/step - loss: 0.2826 - cross_structure: 0.2826\n",
      "Epoch 372/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2817 - cross_structure: 0.2817\n",
      "Epoch 373/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2803 - cross_structure: 0.2803\n",
      "Epoch 374/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2810 - cross_structure: 0.2810\n",
      "Epoch 375/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2821 - cross_structure: 0.2821\n",
      "Epoch 376/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2820 - cross_structure: 0.2820\n",
      "Epoch 377/500\n",
      "19937/19937 [==============================] - 6s 301us/step - loss: 0.2823 - cross_structure: 0.2823\n",
      "Epoch 378/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2810 - cross_structure: 0.2810\n",
      "Epoch 379/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2810 - cross_structure: 0.2810\n",
      "Epoch 380/500\n",
      "19937/19937 [==============================] - 6s 297us/step - loss: 0.2808 - cross_structure: 0.2808\n",
      "Epoch 381/500\n",
      "19937/19937 [==============================] - 6s 300us/step - loss: 0.2805 - cross_structure: 0.2805\n",
      "Epoch 382/500\n",
      "19937/19937 [==============================] - 6s 298us/step - loss: 0.2800 - cross_structure: 0.2800\n",
      "Epoch 383/500\n",
      "19937/19937 [==============================] - 6s 299us/step - loss: 0.2810 - cross_structure: 0.2810\n",
      "Epoch 384/500\n",
      "19937/19937 [==============================] - 6s 302us/step - loss: 0.2806 - cross_structure: 0.2806\n",
      "Epoch 385/500\n",
      "19937/19937 [==============================] - 6s 296us/step - loss: 0.2805 - cross_structure: 0.2805\n",
      "Epoch 386/500\n",
      "19937/19937 [==============================] - 5s 276us/step - loss: 0.2808 - cross_structure: 0.2808\n",
      "Epoch 387/500\n",
      "19937/19937 [==============================] - 5s 274us/step - loss: 0.2796 - cross_structure: 0.2796\n",
      "Epoch 388/500\n",
      "19937/19937 [==============================] - 5s 272us/step - loss: 0.2796 - cross_structure: 0.2796\n",
      "Epoch 389/500\n",
      "19937/19937 [==============================] - 5s 272us/step - loss: 0.2795 - cross_structure: 0.2795\n",
      "Epoch 390/500\n",
      "19937/19937 [==============================] - 5s 275us/step - loss: 0.2794 - cross_structure: 0.2794\n",
      "Epoch 391/500\n",
      "19937/19937 [==============================] - 5s 275us/step - loss: 0.2802 - cross_structure: 0.2802\n",
      "Epoch 392/500\n",
      "19937/19937 [==============================] - 5s 274us/step - loss: 0.2798 - cross_structure: 0.2798\n",
      "Epoch 393/500\n",
      "19937/19937 [==============================] - 5s 275us/step - loss: 0.2787 - cross_structure: 0.2787\n",
      "Epoch 394/500\n",
      "19937/19937 [==============================] - 6s 277us/step - loss: 0.2789 - cross_structure: 0.2789\n",
      "Epoch 395/500\n",
      "19937/19937 [==============================] - 5s 273us/step - loss: 0.2791 - cross_structure: 0.2791\n",
      "Epoch 396/500\n",
      "19937/19937 [==============================] - 5s 272us/step - loss: 0.2792 - cross_structure: 0.2792\n",
      "Epoch 397/500\n",
      "19937/19937 [==============================] - 6s 277us/step - loss: 0.2796 - cross_structure: 0.2796\n",
      "Epoch 398/500\n",
      "19937/19937 [==============================] - 5s 275us/step - loss: 0.2794 - cross_structure: 0.2794\n",
      "Epoch 399/500\n",
      "19937/19937 [==============================] - 6s 276us/step - loss: 0.2795 - cross_structure: 0.2795\n",
      "Epoch 400/500\n",
      "19937/19937 [==============================] - 6s 276us/step - loss: 0.2801 - cross_structure: 0.2801\n",
      "Epoch 401/500\n",
      "19937/19937 [==============================] - 5s 273us/step - loss: 0.2781 - cross_structure: 0.2781\n",
      "Epoch 402/500\n",
      "19937/19937 [==============================] - 6s 277us/step - loss: 0.2788 - cross_structure: 0.2788\n",
      "Epoch 403/500\n",
      "19937/19937 [==============================] - 6s 277us/step - loss: 0.2786 - cross_structure: 0.2786\n",
      "Epoch 404/500\n",
      "19937/19937 [==============================] - 6s 278us/step - loss: 0.2788 - cross_structure: 0.2788\n",
      "Epoch 405/500\n",
      "19937/19937 [==============================] - 6s 277us/step - loss: 0.2785 - cross_structure: 0.2785\n",
      "Epoch 406/500\n",
      "19937/19937 [==============================] - 6s 276us/step - loss: 0.2769 - cross_structure: 0.2769\n",
      "Epoch 407/500\n",
      "19937/19937 [==============================] - 6s 276us/step - loss: 0.2776 - cross_structure: 0.2776\n",
      "Epoch 408/500\n",
      "19937/19937 [==============================] - 6s 277us/step - loss: 0.2772 - cross_structure: 0.2772\n",
      "Epoch 409/500\n",
      "19937/19937 [==============================] - 6s 278us/step - loss: 0.2766 - cross_structure: 0.2766\n",
      "Epoch 410/500\n",
      "19937/19937 [==============================] - 5s 275us/step - loss: 0.2779 - cross_structure: 0.2779\n",
      "Epoch 411/500\n",
      "19937/19937 [==============================] - 6s 276us/step - loss: 0.2757 - cross_structure: 0.2757\n",
      "Epoch 412/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2786 - cross_structure: 0.2786\n",
      "Epoch 413/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2766 - cross_structure: 0.2766\n",
      "Epoch 414/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2764 - cross_structure: 0.2764\n",
      "Epoch 415/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2779 - cross_structure: 0.2779\n",
      "Epoch 416/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2767 - cross_structure: 0.2767\n",
      "Epoch 417/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2770 - cross_structure: 0.2770\n",
      "Epoch 418/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2753 - cross_structure: 0.2753\n",
      "Epoch 419/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2759 - cross_structure: 0.2759\n",
      "Epoch 420/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.2762 - cross_structure: 0.2762\n",
      "Epoch 421/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.2762 - cross_structure: 0.2762\n",
      "Epoch 422/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2755 - cross_structure: 0.2755\n",
      "Epoch 423/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19937/19937 [==============================] - 6s 280us/step - loss: 0.2770 - cross_structure: 0.2770\n",
      "Epoch 424/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2756 - cross_structure: 0.2756\n",
      "Epoch 425/500\n",
      "19937/19937 [==============================] - 6s 280us/step - loss: 0.2748 - cross_structure: 0.2748\n",
      "Epoch 426/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2765 - cross_structure: 0.2765\n",
      "Epoch 427/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2742 - cross_structure: 0.2742\n",
      "Epoch 428/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2759 - cross_structure: 0.2759\n",
      "Epoch 429/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.2744 - cross_structure: 0.2744\n",
      "Epoch 430/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2747 - cross_structure: 0.2747\n",
      "Epoch 431/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2742 - cross_structure: 0.2742\n",
      "Epoch 432/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2759 - cross_structure: 0.2759\n",
      "Epoch 433/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2745 - cross_structure: 0.2745\n",
      "Epoch 434/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2750 - cross_structure: 0.2750\n",
      "Epoch 435/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.2741 - cross_structure: 0.2741\n",
      "Epoch 436/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2733 - cross_structure: 0.2733\n",
      "Epoch 437/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2747 - cross_structure: 0.2747\n",
      "Epoch 438/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2745 - cross_structure: 0.2745\n",
      "Epoch 439/500\n",
      "19937/19937 [==============================] - 6s 286us/step - loss: 0.2751 - cross_structure: 0.2751\n",
      "Epoch 440/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2738 - cross_structure: 0.2738\n",
      "Epoch 441/500\n",
      "19937/19937 [==============================] - 6s 280us/step - loss: 0.2734 - cross_structure: 0.2734\n",
      "Epoch 442/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2738 - cross_structure: 0.2738\n",
      "Epoch 443/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2736 - cross_structure: 0.2736\n",
      "Epoch 444/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2725 - cross_structure: 0.2725\n",
      "Epoch 445/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2735 - cross_structure: 0.2735\n",
      "Epoch 446/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2732 - cross_structure: 0.2732\n",
      "Epoch 447/500\n",
      "19937/19937 [==============================] - ETA: 0s - loss: 0.2732 - cross_structure: 0.273 - 6s 285us/step - loss: 0.2735 - cross_structure: 0.2735\n",
      "Epoch 448/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2727 - cross_structure: 0.2727\n",
      "Epoch 449/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2739 - cross_structure: 0.2739\n",
      "Epoch 450/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2743 - cross_structure: 0.2743\n",
      "Epoch 451/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2726 - cross_structure: 0.2726\n",
      "Epoch 452/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2720 - cross_structure: 0.2720\n",
      "Epoch 453/500\n",
      "19937/19937 [==============================] - 6s 286us/step - loss: 0.2722 - cross_structure: 0.2722\n",
      "Epoch 454/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2729 - cross_structure: 0.2729\n",
      "Epoch 455/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2721 - cross_structure: 0.2721\n",
      "Epoch 456/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2715 - cross_structure: 0.2715\n",
      "Epoch 457/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2719 - cross_structure: 0.2719\n",
      "Epoch 458/500\n",
      "19937/19937 [==============================] - 6s 286us/step - loss: 0.2723 - cross_structure: 0.2723\n",
      "Epoch 459/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2718 - cross_structure: 0.2718\n",
      "Epoch 460/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2720 - cross_structure: 0.2720\n",
      "Epoch 461/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2727 - cross_structure: 0.2727\n",
      "Epoch 462/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2713 - cross_structure: 0.2713\n",
      "Epoch 463/500\n",
      "19937/19937 [==============================] - 6s 288us/step - loss: 0.2708 - cross_structure: 0.2708\n",
      "Epoch 464/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2717 - cross_structure: 0.2717\n",
      "Epoch 465/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2717 - cross_structure: 0.2717\n",
      "Epoch 466/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2718 - cross_structure: 0.2718\n",
      "Epoch 467/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2712 - cross_structure: 0.2712\n",
      "Epoch 468/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2721 - cross_structure: 0.2721\n",
      "Epoch 469/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2713 - cross_structure: 0.2713\n",
      "Epoch 470/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2704 - cross_structure: 0.2704\n",
      "Epoch 471/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2711 - cross_structure: 0.2711\n",
      "Epoch 472/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2710 - cross_structure: 0.2710\n",
      "Epoch 473/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2702 - cross_structure: 0.2702\n",
      "Epoch 474/500\n",
      "19937/19937 [==============================] - 6s 280us/step - loss: 0.2691 - cross_structure: 0.2691\n",
      "Epoch 475/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.2692 - cross_structure: 0.2692\n",
      "Epoch 476/500\n",
      "19937/19937 [==============================] - 6s 287us/step - loss: 0.2713 - cross_structure: 0.2713\n",
      "Epoch 477/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2702 - cross_structure: 0.2702\n",
      "Epoch 478/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2689 - cross_structure: 0.2689\n",
      "Epoch 479/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2703 - cross_structure: 0.2703\n",
      "Epoch 480/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2682 - cross_structure: 0.2682\n",
      "Epoch 481/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2694 - cross_structure: 0.2694\n",
      "Epoch 482/500\n",
      "19937/19937 [==============================] - 6s 280us/step - loss: 0.2695 - cross_structure: 0.2695\n",
      "Epoch 483/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.2699 - cross_structure: 0.2699\n",
      "Epoch 484/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2694 - cross_structure: 0.2694\n",
      "Epoch 485/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2693 - cross_structure: 0.2693\n",
      "Epoch 486/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.2698 - cross_structure: 0.2698\n",
      "Epoch 487/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2684 - cross_structure: 0.2684\n",
      "Epoch 488/500\n",
      "19937/19937 [==============================] - 6s 285us/step - loss: 0.2692 - cross_structure: 0.2692\n",
      "Epoch 489/500\n",
      "19937/19937 [==============================] - 6s 286us/step - loss: 0.2681 - cross_structure: 0.2681\n",
      "Epoch 490/500\n",
      "19937/19937 [==============================] - 6s 287us/step - loss: 0.2694 - cross_structure: 0.2694\n",
      "Epoch 491/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2681 - cross_structure: 0.2681\n",
      "Epoch 492/500\n",
      "19937/19937 [==============================] - 6s 281us/step - loss: 0.2681 - cross_structure: 0.2681\n",
      "Epoch 493/500\n",
      "19937/19937 [==============================] - 6s 286us/step - loss: 0.2674 - cross_structure: 0.2674\n",
      "Epoch 494/500\n",
      "19937/19937 [==============================] - 6s 280us/step - loss: 0.2676 - cross_structure: 0.2676\n",
      "Epoch 495/500\n",
      "19937/19937 [==============================] - 6s 284us/step - loss: 0.2672 - cross_structure: 0.2672\n",
      "Epoch 496/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2665 - cross_structure: 0.2665\n",
      "Epoch 497/500\n",
      "19937/19937 [==============================] - 6s 282us/step - loss: 0.2675 - cross_structure: 0.2675\n",
      "Epoch 498/500\n",
      "19937/19937 [==============================] - 6s 283us/step - loss: 0.2690 - cross_structure: 0.2690\n",
      "Epoch 499/500\n",
      "19937/19937 [==============================] - 6s 288us/step - loss: 0.2669 - cross_structure: 0.2669\n",
      "Epoch 500/500\n",
      "19937/19937 [==============================] - 6s 303us/step - loss: 0.2667 - cross_structure: 0.2667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0c15102690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_model.fit(train_X,cross_train_code_y,nb_epoch=500,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion:\n",
      "rms_error: 17.498046671580198\n",
      "mean_error: 13.128860851152098\n",
      "generating cdf:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression: \n",
      "rms_error: 17.811639847136465\n",
      "mean_error: 13.693871506231945\n",
      "generating cdf:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hierachical classification: \n",
      "rms_error: 18.77541585574408\n",
      "mean_error: 13.713323775688519\n",
      "generating cdf:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "test_AP_features = (np.asarray(test_AP_strengths))/200 + 0.5\n",
    "test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "test_labels = np.asarray(test_df_LL)\n",
    "test_labels,ranges,test_bias =  normalization(test_labels)\n",
    "preds = cross_model.predict(test_AP_features)\n",
    "code = preds[:,:sum(digit)]\n",
    "preds_y_point = []\n",
    "for pred in code:\n",
    "    preds_y_point.append(code_to_point(pred,digit))\n",
    "mlp_reg = preds[:,sum(digit):]\n",
    "pred_pos = 1/2*(1*mlp_reg + 1*preds_y_point)\n",
    "print('fusion:')\n",
    "error_analysis(pred_pos*ranges,test_labels*ranges)\n",
    "print('regression: ')\n",
    "error_analysis(mlp_reg*ranges,test_labels*ranges)\n",
    "print('hierachical classification: ')\n",
    "error_analysis(preds_y_point*ranges,test_labels*ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hirarchical_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-31086ae9fc7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mcdf_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhirarchical_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_AP_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mpreds_y_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hirarchical_model' is not defined"
     ]
    }
   ],
   "source": [
    "#hirarchical_model.compile(optimizer=sgd(learning_rate=0.01), loss=[hirarchical_structure_loss],metrics=['binary_accuracy'])\n",
    "#hirarchical_model.fit(train_X, train_code_y, nb_epoch=200, batch_size=batch_size)\n",
    "def save_to_log(file_name,preds_pos):\n",
    "    write_file = open(file_name,'w')\n",
    "    for pos in preds_pos:\n",
    "        line = str(pos[0])+','+str(pos[1])+'\\n'\n",
    "        write_file.write(line)\n",
    "    return \n",
    "def load_log(file_name):\n",
    "    read_file = open(file_name,'r')\n",
    "    lines = read_file.readlines()\n",
    "    pred_pos = []\n",
    "    for line in lines:\n",
    "        pos = line.split(',')\n",
    "        x = float(pos[0])\n",
    "        y = float(pos[1])\n",
    "        pred_pos.append([x,y])\n",
    "    return pred_pos\n",
    "\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n",
    "\n",
    "def cdf(error):\n",
    "    count = len(error)\n",
    "    cdf_y = [i/count for i in range(count)]\n",
    "    error_sorted = sorted(error)\n",
    "    plt.xlim(0,50)\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(error_sorted, cdf_y)\n",
    "    plt.show()\n",
    "    return cdf_y,error_sorted\n",
    "\n",
    "def error_analysis(pred_y,true_y):\n",
    "    error =np.sqrt((pred_y[:,0]-true_y[:,0])**2+(pred_y[:,1]-true_y[:,1])**2)\n",
    "\n",
    "    rms_error = rms(error)\n",
    "    print('rms_error:', rms_error)\n",
    "    mean_error = sum(error)/len(error)\n",
    "    print('mean_error:', mean_error)\n",
    "    print(\"generating cdf:\")\n",
    "    cdf_y,error_sorted = cdf(error)\n",
    "    return \n",
    "preds = hirarchical_model.predict(test_AP_features)\n",
    "preds_y_point = [] \n",
    "for pred in preds:\n",
    "    preds_y_point.append(code_to_point(pred,digit))\n",
    "preds_result = preds_y_point*ranges+train_bias\n",
    "save_to_log('hierachical_result.csv', preds_result)\n",
    "log_load = load_log('hierachical_result.csv')\n",
    "true_result = test_labels\n",
    "error_analysis(preds_result,true_result)\n",
    "shift = preds_result - true_result\n",
    "shift_x = shift[:,0]\n",
    "shift_y = shift[:,1]\n",
    "#plt.scatter(shift_x, shift_y)\n",
    "plt.scatter(preds_result[:,0],preds_result[:,1],color = 'b',s=5)\n",
    "plt.scatter(true_result[:,0],true_result[:,1],color = 'r',s=5)\n",
    "plt.show()\n",
    "#print(preds_result[0])\n",
    "#print(log_load[0])\n",
    "#print(test_y_return[0])\n",
    "result = []\n",
    "for i in range(len(preds_result)):\n",
    "    y = preds_result[i]\n",
    "    y_hat = test_y_return[i]\n",
    "    error = mean_squared_error(y,y_hat)\n",
    "        #print(y,y_hat,error)\n",
    "    plt.scatter(y[0],y[1],color='y',linewidth=2)\n",
    "    if error < 30:\n",
    "        #print(erlor='b')\n",
    "        result.append(error)\n",
    "    #plt.scatter(y_hat[0],y_hat[1],color='r')\n",
    "    #plt.plot([y[0],y_hat[0]],[y[1],y_hat[1]])\n",
    "#print(preds[1:10],test_labels[1:10])\n",
    "#result = mean_squared_error_index(test_y,preds).numpy()\n",
    "#plt.show()\n",
    "cdf_result = cdf(result)\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,1)\n",
    "plt.plot(cdf_result[0],cdf_result[1])\n",
    "#plt.savefig(\"25%fp,hierarchical,beta =\" + str(i*0.01)+'.png')\n",
    "plt.show()\n",
    "    #errors.append(sum(result)/len(result))\n",
    "print(sum(result)/len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 20\n",
    "batch_size = 10\n",
    "input_size = 520\n",
    "num_classes = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_log(file_name,preds_pos,model_name):\n",
    "    write_file = open(file_name)\n",
    "    for pos in pred_pos:\n",
    "        line = str(pos[0])+','str(pos[1]+'\\n')\n",
    "        write_file.write(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def mean_squared_error_index(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred*ranges/100 - y_true*ranges/100), axis=-1)\n",
    "def encoder():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_size, activation='tanh', bias=True))\n",
    "    model.add(Dense(512, activation='tanh', bias=True))\n",
    "    model.add(Dense(256, activation='tanh', bias=True))\n",
    "    return model\n",
    "def decoder(e):   \n",
    "    e.add(Dense(256, input_dim=64, activation='tanh', bias=True))\n",
    "    e.add(Dense(256, activation='tanh', bias=True))\n",
    "    e.add(Dense(input_size, activation='tanh', bias=True))\n",
    "    e.compile(optimizer='adam', loss='mse')\n",
    "    return e\n",
    "\n",
    "def encoder_mlp(d):\n",
    "    num_to_remove = 3\n",
    "    for i in range(num_to_remove):\n",
    "        d.pop()\n",
    "    d.add(Dense(256, input_dim=64, activation='relu', bias=True))\n",
    "    d.add(Dense(32, activation='relu', bias=True))\n",
    "    d.add(Dense(2, activation='sigmoid', bias=True))\n",
    "    d.compile(optimizer='sgd', loss=[mean_squared_error_index],metrics=[mean_squared_error_index])\n",
    "    return d\n",
    "\n",
    "encoder = encoder()\n",
    "decoder = decoder(encoder)\n",
    "decoder.fit(train_X, train_X, nb_epoch=1, batch_size=batch_size)\n",
    "\n",
    "def regression():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_size, activation='relu', bias=True))\n",
    "    model.add(Dense(512, activation='relu', bias=True))\n",
    "    model.add(Dense(128, activation='relu', bias=True))\n",
    "    model.add(Dense(128, activation='relu', bias=True))\n",
    "    model.add(Dense(32, activation='relu', bias=True))\n",
    "    model.add(Dense(2, activation='sigmoid', bias=True))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=[mean_squared_error_index],metrics=[mean_squared_error_index])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_mlp = encoder_mlp(decoder)\n",
    "encoded_mlp.fit(train_X, train_y,nb_epoch=500, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd(learning_rate=0.005), loss=[mean_squared_error_index],metrics=[mean_squared_error_index])\n",
    "model.fit(train_X, train_y, nb_epoch=500, batch_size=batch_size)\n",
    "loss,accuracy = model.evaluate(test_AP_features, test_labels)\n",
    "print(loss,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import math\n",
    "def cdf(data):\n",
    "    hist, bin_edges = np.histogram(data,bins=100)\n",
    "    cdf = np.cumsum(hist/sum(hist))\n",
    "    return bin_edges[1:],cdf\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n",
    "preds = encoded_mlp.predict(test_AP_features)\n",
    "#print(preds[1:10],test_labels[1:10])\n",
    "test_pos = test_labels\n",
    "preds_pos =  preds*test_ranges+train_bias\n",
    "error = []\n",
    "bias_pos = test_pos - preds_pos\n",
    "test_x = test_pos[:,0]\n",
    "test_y = test_pos[:,1]\n",
    "preds_x = preds_pos[:,0]\n",
    "preds_y = preds_pos[:,1]\n",
    "plt.scatter(test_x, test_y, color='b')\n",
    "plt.scatter(preds_x, preds_y, color='r')\n",
    "plt.show()\n",
    "for bia_pos in bias_pos:\n",
    "    dis = math.sqrt(bia_pos[0]*bia_pos[0]+bia_pos[1]*bia_pos[1])\n",
    "    if dis < 30:\n",
    "        error.append(dis)\n",
    "    \n",
    "    \n",
    "    \n",
    "#result = np.sqrt(result*10000)\n",
    "#cdf_result = cdf(result)\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.plot(cdf_result[0],cdf_result[1])\n",
    "#plt.show()\n",
    "print(\"rms error: \",sum(error)/len(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Final data-type#####\n",
    "### A List indicating the floor_ID and building_ID\n",
    "### A list of dictionaries: database = {'ID':ID,'FPs':FP,'locs':locs,'ts':ts}\n",
    "### FP,locs,ts should be sorted as timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def formatted_fp(df):\n",
    "    df = df.sort_values(by=['TIMESTAMP'])\n",
    "    AP_strengths =df.loc[:,'WAP001':'WAP520']\n",
    "    AP_features = (np.asarray(AP_strengths))/200 + 0.5\n",
    "    building_ids_str = df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    time_stamp = df[\"TIMESTAMP\"]\n",
    "    res = building_ids_str + building_floors_str #element wise concatenation of BUILDINGID+FLOOR\n",
    "    train_labels = np.asarray(building_ids_str + building_floors_str)    \n",
    "    #convert labels to categorical variables, dummy_labels has type 'pandas.core.frame.DataFrame'\n",
    "    dummy_labels = pd.get_dummies(train_labels)\n",
    "    print(df.head(10))\n",
    "    print(time_stamp.head(10))\n",
    "    print(dummy_labels.head(10))\n",
    "formatted_fp(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
