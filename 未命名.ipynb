{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,sgd\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "#train_files,test_files = get_data_files()\n",
    "#print(train_files)\n",
    "def train_val_split(train_AP_features,train_labels,fp_ratio):\n",
    "    #generate len(train_AP_features) of floats in between 0 and 1\n",
    "    train_val_split = np.random.rand(len(train_AP_features))\n",
    "    #convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "    train_val_split = train_val_split < fp_ratio #should contain ~70% percent true\n",
    "    # We will then split our given training set into training + validation \n",
    "    train_X = train_AP_features[train_val_split]\n",
    "    train_y = train_labels[train_val_split]\n",
    "    val_X = train_AP_features[~train_val_split]\n",
    "    val_y = train_labels[~train_val_split]\n",
    "    return train_X,train_y, val_X, val_y\n",
    "\n",
    "def normalization(data):\n",
    "    minVals = data.min(0)\n",
    "    maxVals = data.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normData = (data - minVals)/ranges\n",
    "    return normData,ranges,minVals\n",
    "\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv(file_name,header = 0)\n",
    "    #print(df.head(2))\n",
    "    AP_strengths = df.loc[:,'WAP001':'WAP520']\n",
    "    AP_strengths = AP_strengths.replace([100], [-100])\n",
    "    print(AP_strengths.head(2))\n",
    "    df_xy = df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    labels = np.asarray(df_xy)\n",
    "    AP_features = (np.asarray(AP_strengths))\n",
    "    \n",
    "    building_ids_str = df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    return AP_features, building_ids_str, building_floors_str, labels\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return math.sqrt((y_true[0]-y_pred[0])**2+(y_true[1]-y_pred[1])**2)\n",
    "\n",
    "\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n",
    "def save_to_log(file_name,preds_pos):\n",
    "    write_file = open(file_name,'w')\n",
    "    for pos in preds_pos:\n",
    "        line = str(pos[0])+','+str(pos[1])+'\\n'\n",
    "        write_file.write(line)\n",
    "    return \n",
    "def load_log(file_name):\n",
    "    read_file = open(file_name,'r')\n",
    "    lines = read_file.readlines()\n",
    "    pred_pos = []\n",
    "    for line in lines:\n",
    "        pos = line.split(',')\n",
    "        x = float(pos[0])\n",
    "        y = float(pos[1])\n",
    "        pred_pos.append([x,y])\n",
    "    return pred_pos\n",
    "\n",
    "def cdf(error):\n",
    "    count = len(error)\n",
    "    cdf_y = [i/count for i in range(count)]\n",
    "    error_sorted = sorted(error)\n",
    "    plt.xlim(0,50)\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(error_sorted, cdf_y)\n",
    "    plt.show()\n",
    "    return cdf_y,error_sorted\n",
    "\n",
    "def error_analysis(pred_y,true_y):\n",
    "    error =np.sqrt((pred_y[:,0]-true_y[:,0])**2+(pred_y[:,1]-true_y[:,1])**2)\n",
    "    rms_error = rms(error)\n",
    "    print('rms_error:', rms_error)\n",
    "    mean_error = sum(error)/len(error)\n",
    "    print('mean_error:', mean_error)\n",
    "    print(\"generating cdf:\")\n",
    "    cdf_y,error_sorted = cdf(error)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0    -100    -100    -100    -100    -100    -100    -100    -100    -100   \n",
      "1    -100    -100    -100    -100    -100    -100    -100    -100    -100   \n",
      "\n",
      "   WAP010  ...  WAP511  WAP512  WAP513  WAP514  WAP515  WAP516  WAP517  \\\n",
      "0    -100  ...    -100    -100    -100    -100    -100    -100    -100   \n",
      "1    -100  ...    -100    -100    -100    -100    -100    -100    -100   \n",
      "\n",
      "   WAP518  WAP519  WAP520  \n",
      "0    -100    -100    -100  \n",
      "1    -100    -100    -100  \n",
      "\n",
      "[2 rows x 520 columns]\n"
     ]
    }
   ],
   "source": [
    "AP_features, building_ids_str, building_floors_str, labels = load_data('trainingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input: \n",
    "def mlp_regression():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(512, input_dim=input_size, activation='relu', bias=True))\n",
    "    model1.add(Dense(512, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(32, activation='relu', bias=True))\n",
    "    model1.add(Dense(2, activation='sigmoid', bias=True))\n",
    "    #model1.compile(optimizer='sgd', loss=[mean_squared_error_index],metrics=[mean_squared_error_index])\n",
    "    return model1    \n",
    "\n",
    "def lstm(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    x = L.Reshape((-1, 1))(x)\n",
    "    print(x.shape)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.LSTM(128, dropout=0.1, recurrent_dropout=0, return_sequences=True, activation='sigmoid')(x)\n",
    "    x = L.LSTM(16, dropout=0, return_sequences=False, activation='sigmoid')(x)\n",
    "    #x = L.Dense(128,activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    #x = L.Reshape((-1, 1))(x)\n",
    "    \n",
    "    #x = L.LSTM(16, dropout=0, return_sequences=False, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7541.2643  4.864921e+06      2           1      106   \n",
      "1     100  ...     100 -7536.6212  4.864934e+06      2           1      106   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 2       2       23  1371713733  \n",
      "1                 2       2       23  1371713691  \n",
      "\n",
      "[2 rows x 529 columns]\n",
      "floor id (19937, 1)\n",
      "building id: (19937,)\n",
      "[-97 -94 -94 -94 -90 -90 -88 -88 -87 -86 -86 -84 -83 -83 -83 -80]\n",
      "[154  35 141 155  90  89 102 190 103 150 191 172 171   7 149 247]\n",
      "[[-150 -150 -150 ... -150 -150 -150]\n",
      " [-150 -150 -150 ... -150 -150 -150]\n",
      " [-150 -150 -150 ... -150 -150 -150]\n",
      " ...\n",
      " [-150 -150 -150 ... -150 -150 -150]\n",
      " [-150 -150 -150 ... -150 -150 -150]\n",
      " [-150 -150 -150 ... -150 -150 -150]]\n",
      "(None, None, 1)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 16, 40)       20800       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 40)       160         embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16)           64          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 640)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 640)          10880       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1280)         0           flatten_4[0][0]                  \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1280)         5120        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1280)         0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          327936      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, None, 1)      0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 1)      4           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 1)         13          input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, None, 128)    66560       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 16)           9280        lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1)            4           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 17)           0           lstm_5[0][0]                     \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           288         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "xy (Dense)                      (None, 2)            34          dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 474,039\n",
      "Trainable params: 471,363\n",
      "Non-trainable params: 2,676\n",
      "__________________________________________________________________________________________________\n",
      "[  -7691.3384     4864745.74501597] [390.51940991 270.94278403]\n",
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520    LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7515.916799  4.864890e+06      1           1        0   \n",
      "1     100  ...     100 -7383.867221  4.864840e+06      4           2        0   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 0       0        0  1380872703  \n",
      "1                 0       0       13  1381155054  \n",
      "\n",
      "[2 rows x 529 columns]\n",
      "test floor id (1111, 1)\n",
      "test building id: (1111, 1)\n",
      "[-150 -150 -150 -150 -150 -150 -150 -150 -150 -150 -150 -150 -150 -150\n",
      " -150  -91    1] (1111, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'one hot encode the dummy_labels.\\nthis is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \\nas the column names\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#import tensorflow_addons as tfa\n",
    "#from tensorflow_addons.layers import WeightNormalization\n",
    "#from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "path_train = \"trainingData.csv\"\n",
    "path_validation = \"validationData.csv\"\n",
    "train_df = pd.read_csv(path_train,header = 0)\n",
    "print(train_df.head(2))\n",
    "\n",
    "train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "#train_AP_features= np.array(train_AP_strengths.replace([100], [-100]))\n",
    "building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "floor_enc = LabelEncoder()\n",
    "floor_enc.fit(building_floors_str)\n",
    "floor_id = floor_enc.transform(building_floors_str)\n",
    "floor_id = floor_id.reshape(-1,1)\n",
    "print(\"floor id\",(floor_id.shape))\n",
    "building_enc = LabelEncoder()\n",
    "building_enc.fit(building_ids_str)\n",
    "building_id = building_enc.transform(building_ids_str)\n",
    "train_building_id = building_id.reshape(-1,1)\n",
    "print(\"building id:\",(building_id.shape))\n",
    "\n",
    "train_AP_features = np.array(train_AP_strengths.replace([100],[-150]))\n",
    "##### 1) RSSI_FLOOR 2) SSID 3) BUILDING_ID\n",
    "train_id = np.argsort(train_AP_features)[:,504:520]\n",
    "train_rssi = np.sort(train_AP_features)[:,504:520]\n",
    "print(train_rssi[10])\n",
    "print(train_id[10])\n",
    "print(train_AP_features[train_id[10]])\n",
    "#train_rssi_floor = np.hstack((train_rssi,floor_id))\n",
    "#print(train_rssi_floor[0], train_rssi_floor.shape)\n",
    "#print(train_id[0],train_rssi[0],train_AP_features[0])\n",
    "\n",
    "input_data = [train_id, train_rssi, train_building_id]\n",
    "model = lstm(input_data)\n",
    "model.summary()\n",
    "\n",
    "train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "train_labels = np.asarray(train_df_LL)\n",
    "train_y,ranges,bias =  normalization(train_labels)\n",
    "print(bias,ranges)\n",
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "print(test_df.head(2))\n",
    "test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "#test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "test_floor_enc = LabelEncoder()\n",
    "test_floor_enc.fit(building_floors_str)\n",
    "test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "test_floor_id = test_floor_id.reshape(-1,1)\n",
    "print(\"test floor id\",(test_floor_id.shape))\n",
    "test_building_enc = LabelEncoder()\n",
    "test_building_enc.fit(test_building_ids_str)\n",
    "test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "test_building_id = test_building_id.reshape(-1,1)\n",
    "print(\"test building id:\",(test_building_id.shape))\n",
    "test_AP_features = np.array(test_AP_strengths.replace([100],[-150]))\n",
    "test_id = np.argsort(test_AP_features)[:,504:520]\n",
    "test_rssi = np.sort(test_AP_features)[:,504:520]\n",
    "\n",
    "test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "\n",
    "test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "test_y = np.asarray(test_df_LL)\n",
    "#model.fit(input_data,train_y,nb_epoch=200,batch_size=128,callbacks=[\n",
    "       #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n",
    "#,EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)])\n",
    "#test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "#test_pred_y = test_pred_y * ranges + bias \n",
    "#error_analysis(test_y, test_pred_y)\n",
    "\"\"\"one hot encode the dummy_labels.\n",
    "this is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \n",
    "as the column names\n",
    "\"\"\"\n",
    "#train_X,train_y, val_X, val_y = train_val_split(train_AP_features,train_labels)\n",
    "\n",
    "#Turn the given validation set into a testing set\n",
    "#test_df = pd.read_csv(path_validation,header = 0)\n",
    "#test_AP_features = (np.asarray(test_df.loc[:,'WAP001':'WAP520']))/200+0.5\n",
    "#test_labels = np.asarray(test_df[\"BUILDINGID\"].map(str) + test_df[\"FLOOR\"].map(str))\n",
    "#test_labels = np.asarray(pd.get_dummies(test_labels))\n",
    "#input_size = 520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 19937 samples\n",
      "Epoch 1/10\n",
      "19937/19937 [==============================] - 47s 2ms/sample - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 2/10\n",
      "19937/19937 [==============================] - 41s 2ms/sample - loss: 0.0083 - mse: 0.0083\n",
      "Epoch 3/10\n",
      "19937/19937 [==============================] - 43s 2ms/sample - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 4/10\n",
      "19937/19937 [==============================] - 41s 2ms/sample - loss: 0.0028 - mse: 0.0028\n",
      "Epoch 5/10\n",
      "19937/19937 [==============================] - 40s 2ms/sample - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 6/10\n",
      "19937/19937 [==============================] - 41s 2ms/sample - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 7/10\n",
      "19937/19937 [==============================] - 40s 2ms/sample - loss: 0.0024 - mse: 0.0024\n",
      "Epoch 8/10\n",
      "19937/19937 [==============================] - 40s 2ms/sample - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 9/10\n",
      "19937/19937 [==============================] - 40s 2ms/sample - loss: 0.0022 - mse: 0.0022\n",
      "Epoch 10/10\n",
      "19937/19937 [==============================] - 40s 2ms/sample - loss: 0.0022 - mse: 0.0022\n",
      "rms_error: 12.287968642928108\n",
      "mean_error: 10.053136799950373\n",
      "generating cdf:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 19937 samples\n",
      "Epoch 1/10\n",
      "19937/19937 [==============================] - 42s 2ms/sample - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 2/10\n",
      "19937/19937 [==============================] - 42s 2ms/sample - loss: 0.0020 - mse: 0.0020\n",
      "Epoch 3/10\n",
      " 9984/19937 [==============>...............] - ETA: 20s - loss: 0.0021 - mse: 0.0021"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "for i in range(50):\n",
    "\n",
    "    \n",
    "    #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    model.fit(input_data,train_y,nb_epoch=10,batch_size=128,verbose = 1)\n",
    "    test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "    test_pred_y = test_pred_y * ranges + bias \n",
    "    error_analysis(test_y, test_pred_y)\n",
    "    file = open('lstm_2layer_result.txt','wb')\n",
    "    pickle.dump(test_pred_y,file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    \n",
    "    #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])s\n",
    "    model.fit(input_data,train_y,nb_epoch=10,batch_size=128,verbose = 1)\n",
    "    test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "    test_pred_y = test_pred_y * ranges + bias \n",
    "    error_analysis(test_y, test_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "print(test_df.head(2))\n",
    "test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "#test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "test_floor_enc = LabelEncoder()\n",
    "test_floor_enc.fit(building_floors_str)\n",
    "test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "test_floor_id = test_floor_id.reshape(-1,1)\n",
    "print(\"test floor id\",(test_floor_id.shape))\n",
    "test_building_enc = LabelEncoder()\n",
    "test_building_enc.fit(test_building_ids_str)\n",
    "test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "test_building_id = test_building_id.reshape(-1,1)\n",
    "print(\"test building id:\",(test_building_id.shape))\n",
    "test_AP_features = np.array(test_AP_strengths.replace([100],[-100]))\n",
    "test_id = np.argsort(test_AP_features)[:,500:520]\n",
    "test_rssi = np.sort(test_AP_features)[:,500:520]\n",
    "\n",
    "test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "\n",
    "test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "test_y = np.asarray(test_df_LL)\n",
    "test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "test_pred_y = test_pred_y * ranges + bias \n",
    "error_analysis(test_y, test_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    #print(seq_in, '->', seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length))\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "input_layer = L.Input(shape=(3,))\n",
    "input_layer1 = L.Reshape((1, -1))(input_layer)\n",
    "y = L.LSTM(32,return_sequences=False, activation='relu')(input_layer)\n",
    "output_layer_1 = L.Dense(1, name='output')(y)\n",
    "model = M.Model([input_layer], [output_layer_1])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
