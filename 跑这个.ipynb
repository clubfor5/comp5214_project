{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,sgd\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "#train_files,test_files = get_data_files()\n",
    "#print(train_files)\n",
    "def train_val_split(train_AP_features,train_labels,fp_ratio):\n",
    "    #generate len(train_AP_features) of floats in between 0 and 1\n",
    "    train_val_split = np.random.rand(len(train_AP_features))\n",
    "    #convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "    train_val_split = train_val_split < fp_ratio #should contain ~70% percent true\n",
    "    # We will then split our given training set into training + validation \n",
    "    train_X = train_AP_features[train_val_split]\n",
    "    train_y = train_labels[train_val_split]\n",
    "    val_X = train_AP_features[~train_val_split]\n",
    "    val_y = train_labels[~train_val_split]\n",
    "    return train_X,train_y, val_X, val_y\n",
    "\n",
    "def normalization(data):\n",
    "    minVals = data.min(0)\n",
    "    maxVals = data.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normData = (data - minVals)/ranges\n",
    "    return normData,ranges,minVals\n",
    "\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv(file_name,header = 0)\n",
    "    #print(df.head(2))\n",
    "    AP_strengths = df.loc[:,'WAP001':'WAP520']\n",
    "    AP_strengths = AP_strengths.replace([100], [-100])\n",
    "    print(AP_strengths.head(2))\n",
    "    df_xy = df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    labels = np.asarray(df_xy)\n",
    "    AP_features = (np.asarray(AP_strengths))\n",
    "    \n",
    "    building_ids_str = df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    return AP_features, building_ids_str, building_floors_str, labels\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return math.sqrt((y_true[0]-y_pred[0])**2+(y_true[1]-y_pred[1])**2)\n",
    "\n",
    "\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n",
    "def save_to_log(file_name,preds_pos):\n",
    "    write_file = open(file_name,'w')\n",
    "    for pos in preds_pos:\n",
    "        line = str(pos[0])+','+str(pos[1])+'\\n'\n",
    "        write_file.write(line)\n",
    "    return \n",
    "def load_log(file_name):\n",
    "    read_file = open(file_name,'r')\n",
    "    lines = read_file.readlines()\n",
    "    pred_pos = []\n",
    "    for line in lines:\n",
    "        pos = line.split(',')\n",
    "        x = float(pos[0])\n",
    "        y = float(pos[1])\n",
    "        pred_pos.append([x,y])\n",
    "    return pred_pos\n",
    "\n",
    "def cdf(error):\n",
    "    count = len(error)\n",
    "    cdf_y = [i/count for i in range(count)]\n",
    "    error_sorted = sorted(error)\n",
    "    plt.xlim(0,50)\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(error_sorted, cdf_y)\n",
    "    plt.show()\n",
    "    return cdf_y,error_sorted\n",
    "\n",
    "def error_analysis(pred_y,true_y):\n",
    "    error =np.sqrt((pred_y[:,0]-true_y[:,0])**2+(pred_y[:,1]-true_y[:,1])**2)\n",
    "    rms_error = rms(error)\n",
    "    print('rms_error:', rms_error)\n",
    "    mean_error = sum(error)/len(error)\n",
    "    print('mean_error:', mean_error)\n",
    "    print(\"generating cdf:\")\n",
    "    cdf_y,error_sorted = cdf(error)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(input_data):\n",
    "    print(\"using CNN\")\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    print(\"input_site_layer\", input_site_layer.shape)\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    print(\"site_emb1\", site_emb.shape)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    print(\"site_emb2\", site_emb.shape)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    print(\"site_emb3\", site_emb.shape)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    print(\"before\",x.shape)\n",
    "    x = L.Reshape((128, 1))(x)\n",
    "    print(\"before2\",x.shape)\n",
    "    x = L.BatchNormalization()(x)   # input 128\n",
    "    x = L.Conv1D(32, 3, strides=1, dilation_rate=1, activation='relu')(x)   # input 128, output 126\n",
    "    print(\"CNN1\",x.shape)\n",
    "    y = x\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(64, 5, strides=2, dilation_rate=1, activation='relu')(x)   # input 126, output (126-5+0)/2+1 = 61\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(128, 7, strides=2, dilation_rate=1, activation='relu')(x)  # input 61, output (61-7+0)/2+1 = 28\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(64, 9, strides=1, dilation_rate=1, activation='relu')(x)  # input 23, output (28-9+0)/1+1 = 20\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(32, 5, strides=1, dilation_rate=1, activation='relu')(x)   # input 20, output (20-5+0)/1+1 = 16\n",
    "    print(\"CNN_5 \", x.shape)\n",
    "    x = L.Concatenate(axis=1)([x, y])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    print(\"CNN_res \", x.shape)\n",
    "    x = L.Conv1D(1, 1, strides=1, dilation_rate=1, activation='relu')(x)    # gloabl average pooling\n",
    "    x = L.BatchNormalization()(x)  \n",
    "    print(\"after cnn\", x.shape)\n",
    "    x = L.Flatten()(x)\n",
    "    print(\"after flatten\", x.shape)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(32, activation='relu')(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def cnn_lstm(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    x = L.Reshape((128, 1))(x)\n",
    "    # x = L.Reshape((-1, 1))(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    \n",
    "    x = L.Conv1D(32, 3, strides=1, dilation_rate=1, activation='relu')(x)   # input 128, output 126\n",
    "    y = x\n",
    "    print(\"CNN1\",x.shape)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(64, 5, strides=2, dilation_rate=1, activation='relu')(x)   # input 126, output (126-5+0)/2+1 = 61\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(128, 7, strides=2, dilation_rate=1, activation='relu')(x)  # input 61, output (61-7+0)/2+1 = 28\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(64, 9, strides=1, dilation_rate=1, activation='relu')(x)  # input 23, output (28-9+0)/1+1 = 20\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv1D(32, 5, strides=1, dilation_rate=1, activation='relu')(x)   # input 20, output (20-5+0)/1+1 = 16\n",
    "    x = L.BatchNormalization()(x)\n",
    "    # x = L.Concatenate(axis=1)([x, y])\n",
    "    # x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Conv1D(1, 1, strides=1, dilation_rate=1, activation='relu')(x)    # gloabl average pooling\n",
    "    print(\"after conv1D\", x.shape)\n",
    "    x = L.BatchNormalization()(x) \n",
    "    x = L.LSTM(128, dropout=0, return_sequences=True, activation='sigmoid')(x)\n",
    "    x = L.LSTM(16, dropout=0, return_sequences=False, activation='sigmoid')(x)\n",
    "    print(\"after LSTM \", x.shape)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb]) \n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy', activation='sigmoid')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "def mlp(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    #x = L.Reshape((-1, 1))(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dense(128,activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model\n",
    "def lstm(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,40)(wapid_input_layer)\n",
    "    wap_emb = L.BatchNormalization()(wap_emb)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(16*40, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    site_emb = L.BatchNormalization()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    #x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.1)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    x = L.Reshape((-1, 1))(x)\n",
    "    print(x.shape)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.LSTM(128, dropout=0.1, recurrent_dropout=0, return_sequences=True, activation='sigmoid')(x)\n",
    "    x = L.LSTM(64, dropout=0.1, return_sequences=False, activation='sigmoid')(x)\n",
    "    #x = L.Dense(128,activation='relu')(x)\n",
    "    #x = L.Dropout(0.2)(x)\n",
    "    #x = L.Reshape((-1, 1))(x)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    #x = L.LSTM(16, dropout=0, return_sequences=False, activation='relu')(x)\n",
    "    x = L.Concatenate(axis=1)([x,site_emb])\n",
    "    x = L.Dense(16, activation='relu')(x)\n",
    "    #x = L.Dropout(0.1)(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def altered_rssi_analysis():\n",
    "    from sklearn import neighbors\n",
    "    path_train = \"trainingData.csv\"\n",
    "    path_validation = \"validationData.csv\"\n",
    "    train_df = pd.read_csv(path_train,header = 0)\n",
    "    train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "    building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    floor_enc = LabelEncoder()\n",
    "    floor_enc.fit(building_floors_str)\n",
    "    floor_id = floor_enc.transform(building_floors_str)\n",
    "    floor_id = floor_id.reshape(-1,1)\n",
    "    building_enc = LabelEncoder()\n",
    "    building_enc.fit(building_ids_str)\n",
    "    building_id = building_enc.transform(building_ids_str)\n",
    "    train_building_id = building_id.reshape(-1,1)\n",
    "    train_AP_features = np.array(train_AP_strengths.replace([100],[-150]))\n",
    "    train_id = np.argsort(train_AP_features)[:,504:520]\n",
    "    train_rssi = np.sort(train_AP_features)[:,504:520]\n",
    "    print(train_rssi[10])\n",
    "    print(train_id[10])\n",
    "    print(train_AP_features[train_id[10]])\n",
    "    train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    train_labels = np.asarray(train_df_LL)\n",
    "    train_y,ranges,bias =  normalization(train_labels)\n",
    "    print(bias,ranges)\n",
    "    test_df = pd.read_csv(path_validation,header = 0)\n",
    "    print(test_df.head(2))\n",
    "    test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "    #test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "    test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    #print(id_label)\n",
    "    test_floor_enc = LabelEncoder()\n",
    "    test_floor_enc.fit(building_floors_str)\n",
    "    test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "    test_floor_id = test_floor_id.reshape(-1,1)\n",
    "    print(\"test floor id\",(test_floor_id.shape))\n",
    "    test_building_enc = LabelEncoder()\n",
    "    test_building_enc.fit(test_building_ids_str)\n",
    "    test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "    test_building_id = test_building_id.reshape(-1,1)\n",
    "    print(\"test building id:\",(test_building_id.shape))\n",
    "    test_AP_features = np.array(test_AP_strengths.replace([100],[-150]))\n",
    "    test_id = np.argsort(test_AP_features)[:,504:520]\n",
    "    test_rssi = np.sort(test_AP_features)[:,504:520]\n",
    "    test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "    print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "    test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    test_y = np.asarray(test_df_LL)\n",
    "    for i in range(1,6):\n",
    "        ratio = i * 2\n",
    "        train_rssi =  train_rssi + np.random.normal(0, 2*i, None)\n",
    "        input_data = [train_id, train_rssi, train_building_id]\n",
    "\n",
    "        model = cnn_lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_lstm_layer_altered_' + str(2*i)+ '_db.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        \n",
    "        model = lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'lstm_layer_altered_' + str(2*i)+ '_db.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        \n",
    "        # CNN\n",
    "        model = cnn(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_layer_altered_' + str(i*2)+ '_db.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        # MLP\n",
    "\n",
    "        model = mlp(input_data)\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'mlp_layer_altered_' + str(ratio)+ '_db.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "        #LSTM\n",
    "\n",
    "\n",
    "#altered_rssi_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_rssi_analysis():\n",
    "    from sklearn import neighbors\n",
    "    path_train = \"trainingData.csv\"\n",
    "    path_validation = \"validationData.csv\"\n",
    "    train_df = pd.read_csv(path_train,header = 0)\n",
    "    train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "    building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    floor_enc = LabelEncoder()\n",
    "    floor_enc.fit(building_floors_str)\n",
    "    floor_id = floor_enc.transform(building_floors_str)\n",
    "    floor_id = floor_id.reshape(-1,1)\n",
    "    building_enc = LabelEncoder()\n",
    "    building_enc.fit(building_ids_str)\n",
    "    building_id = building_enc.transform(building_ids_str)\n",
    "    train_building_id = building_id.reshape(-1,1)\n",
    "    train_AP_features = np.array(train_AP_strengths.replace([100],[-150]))\n",
    "    \n",
    "    \n",
    "    train_id = np.argsort(train_AP_features)[:,504:520]\n",
    "    train_rssi = np.sort(train_AP_features)[:,504:520]\n",
    "    print(train_rssi[10])\n",
    "    print(train_id[10])\n",
    "    print(train_AP_features[train_id[10]])\n",
    "    train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    train_labels = np.asarray(train_df_LL)\n",
    "    train_y,ranges,bias =  normalization(train_labels)\n",
    "    print(bias,ranges)\n",
    "    test_df = pd.read_csv(path_validation,header = 0)\n",
    "    print(test_df.head(2))\n",
    "    test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "    #test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "    test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    #print(id_label)\n",
    "    test_floor_enc = LabelEncoder()\n",
    "    test_floor_enc.fit(building_floors_str)\n",
    "    test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "    test_floor_id = test_floor_id.reshape(-1,1)\n",
    "    print(\"test floor id\",(test_floor_id.shape))\n",
    "    test_building_enc = LabelEncoder()\n",
    "    test_building_enc.fit(test_building_ids_str)\n",
    "    test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "    test_building_id = test_building_id.reshape(-1,1)\n",
    "    print(\"test building id:\",(test_building_id.shape))\n",
    "    test_AP_features = np.array(test_AP_strengths.replace([100],[-150]))\n",
    "    test_id = np.argsort(test_AP_features)[:,504:520]\n",
    "    test_rssi = np.sort(test_AP_features)[:,504:520]\n",
    "    test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "    print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "    test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    test_y = np.asarray(test_df_LL)\n",
    "    for i in [9]:\n",
    "        arr = np.random.random(size=train_rssi.shape)\n",
    "        mask = arr < i*0.1\n",
    "        print(mask)\n",
    "        train_rssi[mask] = -150 # 把标记为True的值记为0\n",
    "        print(train_rssi)\n",
    "        input_data = [train_id, train_rssi, train_building_id]\n",
    "\n",
    "\n",
    "        #LSTM\n",
    "        model = cnn_lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_lstm_layer_altered_' + str(i*0.1)+ '_ratio.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        model = lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'lstm_layer_altered_' + str(i*0.1)+ '_ratio.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        \n",
    "        # CNN\n",
    "        model = cnn(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_layer_altered_' + str(i*0.1)+ '__ratio.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        # MLP\n",
    "\n",
    "\n",
    "#masked_rssi_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 9, 11, 13, 15, 17, 18, 21, 22, 24, 25, 29, 33, 39, 45, 46, 47, 49, 50, 57, 58, 59, 60, 62, 64, 65, 67, 69, 72, 73, 74, 76, 85, 86, 89, 92, 100, 102, 103, 104, 108, 110, 115, 116, 117, 118, 120, 124, 128, 132, 137, 141, 143, 152, 159, 160, 161, 162, 164, 175, 179, 183, 185, 186, 187, 189, 191, 196, 197, 200, 202, 209, 213, 217, 219, 220, 225, 227, 232, 235, 236, 239, 249, 253, 255, 256, 262, 264, 269, 278, 279, 280, 286, 293, 294, 295, 303, 305, 307, 309, 312, 316, 328, 329, 330, 336, 337, 338, 339, 341, 347, 348, 355, 356, 360, 364, 365, 367, 369, 370, 373, 374, 376, 385, 386, 388, 390, 393, 398, 399, 400, 401, 402, 403, 405, 410, 412, 415, 417, 418, 419, 421, 423, 424, 425, 429, 430, 431, 444, 452, 453, 455, 457, 458, 468, 469, 471, 472, 473, 477, 481, 484, 485, 486, 489, 492, 502, 506, 507, 510, 512, 514, 516]\n",
      "(19937, 4)\n",
      "(1111, 4)\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 4, 40)        20800       input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 40)        160         embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 4)            16          input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 160)          0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 640)          3200        batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 800)          0           flatten_14[0][0]                 \n",
      "                                                                 dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 800)          3200        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 800)          0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 256)          205056      dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 256)          0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 256)          1024        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 128)          32896       batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 1)         13          input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 128)          0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 1)            0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 64)           8256        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 1)            4           flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 65)           0           dense_38[0][0]                   \n",
      "                                                                 batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 16)           1056        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "xy (Dense)                      (None, 2)            34          dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 275,715\n",
      "Trainable params: 273,513\n",
      "Non-trainable params: 2,202\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 19937 samples\n",
      "Epoch 1/100\n",
      "19937/19937 [==============================] - 3s 138us/sample - loss: 0.0364 - mse: 0.0364\n",
      "Epoch 2/100\n",
      "19937/19937 [==============================] - 2s 80us/sample - loss: 0.0097 - mse: 0.0097\n",
      "Epoch 3/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0061 - mse: 0.0061\n",
      "Epoch 4/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 5/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0042 - mse: 0.0042\n",
      "Epoch 6/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0035 - mse: 0.0035\n",
      "Epoch 7/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 8/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 0.0028 - mse: 0.0028\n",
      "Epoch 9/100\n",
      "19937/19937 [==============================] - 2s 84us/sample - loss: 0.0026 - mse: 0.0026\n",
      "Epoch 10/100\n",
      "19937/19937 [==============================] - 2s 87us/sample - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 11/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0024 - mse: 0.0024\n",
      "Epoch 12/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0022 - mse: 0.0022\n",
      "Epoch 13/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0021 - mse: 0.0021\n",
      "Epoch 14/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 15/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0020 - mse: 0.0020\n",
      "Epoch 16/100\n",
      "19937/19937 [==============================] - 2s 84us/sample - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 17/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 18/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 19/100\n",
      "19937/19937 [==============================] - 2s 84us/sample - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19937/19937 [==============================] - 2s 83us/sample - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 21/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0016 - mse: 0.0016\n",
      "Epoch 22/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 23/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 24/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 25/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 26/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 27/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 28/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 29/100\n",
      "19937/19937 [==============================] - 2s 80us/sample - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 30/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 31/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 32/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 33/100\n",
      "19937/19937 [==============================] - 2s 86us/sample - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 34/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 35/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 36/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 37/100\n",
      "19937/19937 [==============================] - 2s 84us/sample - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 38/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 39/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 40/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 41/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 42/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 43/100\n",
      "19937/19937 [==============================] - 2s 84us/sample - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 44/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 45/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 46/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 47/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 9.9479e-04 - mse: 9.9479e-04\n",
      "Epoch 48/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 49/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 9.9931e-04 - mse: 9.9931e-04\n",
      "Epoch 50/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 51/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 9.6378e-04 - mse: 9.6378e-04\n",
      "Epoch 52/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 9.4519e-04 - mse: 9.4519e-04\n",
      "Epoch 53/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 9.5227e-04 - mse: 9.5227e-04\n",
      "Epoch 54/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 9.2517e-04 - mse: 9.2517e-04\n",
      "Epoch 55/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 8.8420e-04 - mse: 8.8420e-04\n",
      "Epoch 56/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 9.0164e-04 - mse: 9.0164e-04\n",
      "Epoch 57/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 9.0081e-04 - mse: 9.0081e-04\n",
      "Epoch 58/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 8.9673e-04 - mse: 8.9673e-04\n",
      "Epoch 59/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 8.5454e-04 - mse: 8.5454e-04\n",
      "Epoch 60/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 8.3904e-04 - mse: 8.3904e-04\n",
      "Epoch 61/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 8.9302e-04 - mse: 8.9302e-04\n",
      "Epoch 62/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 8.8575e-04 - mse: 8.8575e-04\n",
      "Epoch 63/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 8.5362e-04 - mse: 8.5362e-04\n",
      "Epoch 64/100\n",
      "19937/19937 [==============================] - 2s 80us/sample - loss: 8.3907e-04 - mse: 8.3907e-04\n",
      "Epoch 65/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 7.9102e-04 - mse: 7.9102e-04\n",
      "Epoch 66/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 8.1477e-04 - mse: 8.1477e-04\n",
      "Epoch 67/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 7.7137e-04 - mse: 7.7137e-04\n",
      "Epoch 68/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 7.6648e-04 - mse: 7.6648e-04\n",
      "Epoch 69/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 7.8756e-04 - mse: 7.8756e-04\n",
      "Epoch 70/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 7.9141e-04 - mse: 7.9141e-04\n",
      "Epoch 71/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 7.8540e-04 - mse: 7.8540e-04\n",
      "Epoch 72/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 7.3876e-04 - mse: 7.3876e-04\n",
      "Epoch 73/100\n",
      "19937/19937 [==============================] - 2s 88us/sample - loss: 7.3421e-04 - mse: 7.3421e-04\n",
      "Epoch 74/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 7.7889e-04 - mse: 7.7889e-04\n",
      "Epoch 75/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 7.8353e-04 - mse: 7.8353e-04\n",
      "Epoch 76/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 7.5634e-04 - mse: 7.5634e-04\n",
      "Epoch 77/100\n",
      "19937/19937 [==============================] - 2s 80us/sample - loss: 7.4194e-04 - mse: 7.4194e-04\n",
      "Epoch 78/100\n",
      "19937/19937 [==============================] - 2s 84us/sample - loss: 7.0995e-04 - mse: 7.0995e-04\n",
      "Epoch 79/100\n",
      "19937/19937 [==============================] - 2s 87us/sample - loss: 7.1077e-04 - mse: 7.1077e-04\n",
      "Epoch 80/100\n",
      "19937/19937 [==============================] - 2s 85us/sample - loss: 6.8591e-04 - mse: 6.8591e-04\n",
      "Epoch 81/100\n",
      "19937/19937 [==============================] - 2s 85us/sample - loss: 6.9242e-04 - mse: 6.9242e-04\n",
      "Epoch 82/100\n",
      "19937/19937 [==============================] - 2s 89us/sample - loss: 6.9805e-04 - mse: 6.9805e-04\n",
      "Epoch 83/100\n",
      "19937/19937 [==============================] - 2s 85us/sample - loss: 6.9861e-04 - mse: 6.9861e-04\n",
      "Epoch 84/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 6.8634e-04 - mse: 6.8634e-04\n",
      "Epoch 85/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 6.7295e-04 - mse: 6.7295e-04\n",
      "Epoch 86/100\n",
      "19937/19937 [==============================] - 2s 92us/sample - loss: 6.7797e-04 - mse: 6.7797e-04\n",
      "Epoch 87/100\n",
      "19937/19937 [==============================] - 2s 87us/sample - loss: 6.5131e-04 - mse: 6.5131e-04\n",
      "Epoch 88/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 6.6912e-04 - mse: 6.6912e-04\n",
      "Epoch 89/100\n",
      "19937/19937 [==============================] - 2s 88us/sample - loss: 6.6634e-04 - mse: 6.6634e-04\n",
      "Epoch 90/100\n",
      "19937/19937 [==============================] - 2s 89us/sample - loss: 6.7188e-04 - mse: 6.7188e-04\n",
      "Epoch 91/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 6.4836e-04 - mse: 6.4836e-04\n",
      "Epoch 92/100\n",
      "19937/19937 [==============================] - 2s 84us/sample - loss: 6.5713e-04 - mse: 6.5713e-04\n",
      "Epoch 93/100\n",
      "19937/19937 [==============================] - 2s 83us/sample - loss: 6.5154e-04 - mse: 6.5154e-04\n",
      "Epoch 94/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 6.4801e-04 - mse: 6.4801e-04\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19937/19937 [==============================] - 2s 81us/sample - loss: 6.4667e-04 - mse: 6.4667e-04\n",
      "Epoch 96/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 6.2216e-04 - mse: 6.2216e-04\n",
      "Epoch 97/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 6.2783e-04 - mse: 6.2783e-04\n",
      "Epoch 98/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 6.1635e-04 - mse: 6.1635e-04\n",
      "Epoch 99/100\n",
      "19937/19937 [==============================] - 2s 82us/sample - loss: 6.3723e-04 - mse: 6.3723e-04\n",
      "Epoch 100/100\n",
      "19937/19937 [==============================] - 2s 81us/sample - loss: 6.2804e-04 - mse: 6.2804e-04\n",
      "rms_error: 18.45898880524672\n",
      "mean_error: 13.313787157927305\n",
      "generating cdf:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file name:  mlp_layer_ap_ratio0.3.txt\n",
      "CNN1 (None, 126, 32)\n",
      "after conv1D (None, 16, 1)\n",
      "after LSTM  (None, 16)\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2325\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'batch_normalization_64/cond_1' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2329\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'batch_normalization_64/cond_1' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-73069f758c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m#model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rssi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_building_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pred_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mranges\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[0;31m# function we recompile the metrics based on the updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m   \u001b[0;31m# sample_weight_mode value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;31m# Prepare validation data. Hold references to the iterator and the input list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m    553\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2029\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1972\u001b[0m           \u001b[0;31m# Training updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m           updates = self.optimizer.get_updates(\n\u001b[0;32m-> 1974\u001b[0;31m               params=self._collected_trainable_weights, loss=self.total_loss)\n\u001b[0m\u001b[1;32m   1975\u001b[0m       \u001b[0;31m# Unconditional updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_updates_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     self._assert_valid_dtypes([\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scope_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 677\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    678\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    347\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 677\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    678\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_IfGrad\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;31m# Get the if operator (this logic handles the case where op is a MockOp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0mif_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   \u001b[0mtrue_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_func_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mif_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m   \u001b[0;31m# Note: op.graph != ops.get_default_graph() when we are computing the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# of a nested cond.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_get_func_graphs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"If\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     return (_get_func_graph_for_branch(op.get_attr(\"then_branch\")),\n\u001b[0;32m--> 269\u001b[0;31m             _get_func_graph_for_branch(op.get_attr(\"else_branch\")))\n\u001b[0m\u001b[1;32m    270\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Case\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     return [_get_func_graph_for_branch(branch_fn)\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_get_func_graph_for_branch\u001b[0;34m(name_attr_list)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m       func_graph = function_def_to_graph.function_def_to_graph(\n\u001b[0;32m--> 258\u001b[0;31m           fdef, input_shapes)\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexternal_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_t\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0mcustom_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexternal_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, input_shapes)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Add all function nodes to the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Initialize fields specific to FuncGraph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    422\u001b[0m   \u001b[0;31m# _ProcessNewOps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def select_random_columns(ratio):\n",
    "    select_columns = []\n",
    "    for i in range(520):\n",
    "        if np.random.random() < ratio:\n",
    "            select_columns.append(i)\n",
    "    return select_columns\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sklearn import neighbors\n",
    "    ratios = [0.3,0.7,0.5,0.3,0.1]\n",
    "    for ratio in ratios:\n",
    "        columns = select_random_columns(ratio)\n",
    "        print(columns)\n",
    "        path_train = \"trainingData.csv\"\n",
    "        path_validation = \"validationData.csv\"\n",
    "        train_df = pd.read_csv(path_train,header = 0)\n",
    "        train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "        building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "        building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "        floor_enc = LabelEncoder()\n",
    "        floor_enc.fit(building_floors_str)\n",
    "        floor_id = floor_enc.transform(building_floors_str)\n",
    "        floor_id = floor_id.reshape(-1,1)\n",
    "        building_enc = LabelEncoder()\n",
    "        building_enc.fit(building_ids_str)\n",
    "        building_id = building_enc.transform(building_ids_str)\n",
    "        train_building_id = building_id.reshape(-1,1)\n",
    "        train_AP_features = np.array(train_AP_strengths.replace([100],[-150]))   \n",
    "        train_AP_features = train_AP_features[:,columns]\n",
    "        \n",
    "        dimensions = train_AP_features.shape[1]\n",
    "        train_id = np.argsort(train_AP_features)[:,dimensions-int(16*ratio):dimensions]\n",
    "        train_rssi = np.sort(train_AP_features)[:,dimensions-int(16*ratio):dimensions]\n",
    "        print(train_rssi.shape)\n",
    "        #print(train_rssi[10])\n",
    "        #print(train_id[10])\n",
    "        #print(train_AP_features[train_id[10]])\n",
    "        train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "        train_labels = np.asarray(train_df_LL)\n",
    "        train_y,ranges,bias =  normalization(train_labels)\n",
    "        #print(bias,ranges)\n",
    "        test_df = pd.read_csv(path_validation,header = 0)\n",
    "        #print(test_df.head(2))\n",
    "        test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "        #test_AP_features = np.array(test_AP_strengths.replace([100], [-100]))\n",
    "        test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "        test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "        #print(id_label)\n",
    "        test_floor_enc = LabelEncoder()\n",
    "        test_floor_enc.fit(building_floors_str)\n",
    "        test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "        test_floor_id = test_floor_id.reshape(-1,1)\n",
    "        #print(\"test floor id\",(test_floor_id.shape))\n",
    "        test_building_enc = LabelEncoder()\n",
    "        test_building_enc.fit(test_building_ids_str)\n",
    "        test_building_id = test_building_enc.transform(test_building_ids_str)\n",
    "        test_building_id = test_building_id.reshape(-1,1)\n",
    "        #print(\"test building id:\",(test_building_id.shape))\n",
    "        test_AP_features = np.array(test_AP_strengths.replace([100],[-150]))\n",
    "        test_AP_features = test_AP_features[:,columns]\n",
    "        test_id = np.argsort(test_AP_features)[:,dimensions-int(16*ratio):dimensions]\n",
    "        test_rssi = np.sort(test_AP_features)[:,dimensions-int(16*ratio):dimensions]\n",
    "        print(test_rssi.shape)\n",
    "        test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "        #print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "        test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "        test_y = np.asarray(test_df_LL)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        input_data = [train_id, train_rssi, train_building_id]\n",
    "        #LSTM\n",
    "        \n",
    "        \n",
    "        model = mlp(input_data)\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'mlp_layer_ap_ratio' + str(ratio)+ '.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        model = cnn_lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_lstm_layer_ap_ratio' + str(i*0.1)+ '.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        model = lstm(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'lstm_layer_ap_ratio' + str(i*0.1)+ '.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "        \n",
    "        # CNN\n",
    "        model = cnn(input_data)\n",
    "        #model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "        model.fit(input_data,train_y,nb_epoch=100,batch_size=128,verbose = 1)\n",
    "        test_pred_y = model.predict([test_id, test_rssi, test_building_id])\n",
    "        test_pred_y = test_pred_y * ranges + bias \n",
    "        error_analysis(test_y, test_pred_y)\n",
    "        fileName = 'cnn_layer_ap_ratio' + str(i*0.1)+ '.txt'\n",
    "        print(\"writing to file name: \", fileName)\n",
    "        file = open(fileName,'wb')\n",
    "        pickle.dump(test_pred_y,file)\n",
    "        file.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
