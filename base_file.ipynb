{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam,sgd\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "#train_files,test_files = get_data_files()\n",
    "#print(train_files)\n",
    "def train_val_split(train_AP_features,train_labels,fp_ratio):\n",
    "    #generate len(train_AP_features) of floats in between 0 and 1\n",
    "    train_val_split = np.random.rand(len(train_AP_features))\n",
    "    #convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "    train_val_split = train_val_split < fp_ratio #should contain ~70% percent true\n",
    "    # We will then split our given training set into training + validation \n",
    "    train_X = train_AP_features[train_val_split]\n",
    "    train_y = train_labels[train_val_split]\n",
    "    val_X = train_AP_features[~train_val_split]\n",
    "    val_y = train_labels[~train_val_split]\n",
    "    return train_X,train_y, val_X, val_y\n",
    "\n",
    "def normalization(data):\n",
    "    minVals = data.min(0)\n",
    "    maxVals = data.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normData = (data - minVals)/ranges\n",
    "    return normData,ranges,minVals\n",
    "\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv(file_name,header = 0)\n",
    "    #print(df.head(2))\n",
    "    AP_strengths = df.loc[:,'WAP001':'WAP520']\n",
    "    AP_strengths = AP_strengths.replace([100], [-100])\n",
    "    print(AP_strengths.head(2))\n",
    "    df_xy = df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    labels = np.asarray(df_xy)\n",
    "    AP_features = (np.asarray(AP_strengths))\n",
    "    \n",
    "    building_ids_str = df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    return AP_features, building_ids_str, building_floors_str, labels\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return math.sqrt((y_true[0]-y_pred[0])**2+(y_true[1]-y_pred[1])**2)\n",
    "\n",
    "\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n",
    "def save_to_log(file_name,preds_pos):\n",
    "    write_file = open(file_name,'w')\n",
    "    for pos in preds_pos:\n",
    "        line = str(pos[0])+','+str(pos[1])+'\\n'\n",
    "        write_file.write(line)\n",
    "    return \n",
    "def load_log(file_name):\n",
    "    read_file = open(file_name,'r')\n",
    "    lines = read_file.readlines()\n",
    "    pred_pos = []\n",
    "    for line in lines:\n",
    "        pos = line.split(',')\n",
    "        x = float(pos[0])\n",
    "        y = float(pos[1])\n",
    "        pred_pos.append([x,y])\n",
    "    return pred_pos\n",
    "\n",
    "def cdf(error):\n",
    "    count = len(error)\n",
    "    cdf_y = [i/count for i in range(count)]\n",
    "    error_sorted = sorted(error)\n",
    "    plt.xlim(0,50)\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(error_sorted, cdf_y)\n",
    "    plt.show()\n",
    "    return cdf_y,error_sorted\n",
    "\n",
    "def error_analysis(pred_y,true_y):\n",
    "    error =np.sqrt((pred_y[:,0]-true_y[:,0])**2+(pred_y[:,1]-true_y[:,1])**2)\n",
    "\n",
    "    rms_error = rms(error)\n",
    "    print('rms_error:', rms_error)\n",
    "    mean_error = sum(error)/len(error)\n",
    "    print('mean_error:', mean_error)\n",
    "    print(\"generating cdf:\")\n",
    "    cdf_y,error_sorted = cdf(error)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0    -100    -100    -100    -100    -100    -100    -100    -100    -100   \n",
      "1    -100    -100    -100    -100    -100    -100    -100    -100    -100   \n",
      "\n",
      "   WAP010  ...  WAP511  WAP512  WAP513  WAP514  WAP515  WAP516  WAP517  \\\n",
      "0    -100  ...    -100    -100    -100    -100    -100    -100    -100   \n",
      "1    -100  ...    -100    -100    -100    -100    -100    -100    -100   \n",
      "\n",
      "   WAP518  WAP519  WAP520  \n",
      "0    -100    -100    -100  \n",
      "1    -100    -100    -100  \n",
      "\n",
      "[2 rows x 520 columns]\n"
     ]
    }
   ],
   "source": [
    "AP_features, building_ids_str, building_floors_str, labels = load_data('trainingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input: \n",
    "def mlp_regression():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(512, input_dim=input_size, activation='relu', bias=True))\n",
    "    model1.add(Dense(512, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(32, activation='relu', bias=True))\n",
    "    model1.add(Dense(2, activation='sigmoid', bias=True))\n",
    "    #model1.compile(optimizer='sgd', loss=[mean_squared_error_index],metrics=[mean_squared_error_index])\n",
    "    return model1    \n",
    "\n",
    "def lstm(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,64)(wapid_input_layer)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(64, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature, site_emb])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Reshape((-1, 1))(x)\n",
    "    print(x.shape)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.LSTM(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, activation='relu')(x)\n",
    "    x = L.LSTM(16, dropout=0.1, return_sequences=False, activation='relu')(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7541.2643  4.864921e+06      2           1      106   \n",
      "1     100  ...     100 -7536.6212  4.864934e+06      2           1      106   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 2       2       23  1371713733  \n",
      "1                 2       2       23  1371713691  \n",
      "\n",
      "[2 rows x 529 columns]\n",
      "floor id [[2]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n",
      "building id: [1 1 1 ... 1 1 1]\n",
      "[0.025 0.04  0.045 0.045 0.055 0.085 0.09  0.095 0.095 0.105 0.11  0.16\n",
      " 0.16  0.16  0.165 0.23  0.235 1.    1.    1.    1.    1.    1.    1.\n",
      " 1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.\n",
      " 1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.\n",
      " 1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.\n",
      " 1.    1.    1.    1.    2.   ] (19937, 65)\n",
      "(None, None, 1)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 65)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 64, 64)       33280       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 65)           260         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 1)         13          input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 4096)         0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4224        batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1)            0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4161)         0           flatten_8[0][0]                  \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4161)         16644       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 4161)         0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          1065472     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, None, 1)      0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 1)      4           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, None, 256)    264192      batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 16)           17472       lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "xy (Dense)                      (None, 2)            34          lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,401,595\n",
      "Trainable params: 1,393,141\n",
      "Non-trainable params: 8,454\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 19937 samples\n",
      "Epoch 1/500\n",
      "  512/19937 [..............................] - ETA: 10:17 - loss: 0.3091 - mse: 0.3091"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e542531fa408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_LL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mranges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#import tensorflow_addons as tfa\n",
    "#from tensorflow_addons.layers import WeightNormalization\n",
    "#from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "path_train = \"trainingData.csv\"\n",
    "path_validation = \"validationData.csv\"\n",
    "train_df = pd.read_csv(path_train,header = 0)\n",
    "print(train_df.head(2))\n",
    "\n",
    "train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "train_AP_features = (np.asarray(train_AP_strengths))/200 + 0.5\n",
    "building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "floor_enc = LabelEncoder()\n",
    "floor_enc.fit(building_floors_str)\n",
    "floor_id = floor_enc.transform(building_floors_str)\n",
    "floor_id = floor_id.reshape(-1,1)\n",
    "print(\"floor id\",(floor_id))\n",
    "building_enc = LabelEncoder()\n",
    "building_enc.fit(building_ids_str)\n",
    "building_id = building_enc.transform(building_ids_str)\n",
    "train_building_id = building_id.reshape(-1,1)\n",
    "print(\"building id:\",(building_id))\n",
    "\n",
    "\n",
    "##### 1) RSSI_FLOOR 2) SSID 3) BUILDING_ID\n",
    "train_id = np.argsort(train_AP_features)[:,0:64]\n",
    "train_rssi = np.sort(train_AP_features)[:,0:64]\n",
    "\n",
    "train_rssi_floor = np.hstack((train_rssi,floor_id))\n",
    "print(train_rssi_floor[0], train_rssi_floor.shape)\n",
    "#print(train_id[0],train_rssi[0],train_AP_features[0])\n",
    "\n",
    "input_data = [train_id, train_rssi_floor, train_building_id]\n",
    "model = lstm(input_data)\n",
    "model.summary()\n",
    "\n",
    "train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "train_labels = np.asarray(train_df_LL)\n",
    "train_y,ranges,bias =  normalization(train_labels)\n",
    "model.fit(input_data,train_y,nb_epoch=500,batch_size=128)\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "print(test_df.head(2))\n",
    "test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "test_AP_features = (np.asarray(test_AP_strengths))/200 + 0.5\n",
    "test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "test_floor_enc = LabelEncoder()\n",
    "test_floor_enc.fit(building_floors_str)\n",
    "test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "test_floor_id = test_floor_id.reshape(-1,1)\n",
    "print(\"test floor id\",(test_floor_id))\n",
    "test_building_enc = LabelEncoder()\n",
    "test_building_enc.fit(test_building_ids_str)\n",
    "test_building_id = test_building_enc.transform(building_ids_str)\n",
    "test_building_id = test_building_id.reshape(-1,1)\n",
    "print(\"test building id:\",(test_building_id))\n",
    "test_id = np.argsort(test_AP_features)[:,0:64]\n",
    "test_rssi = np.sort(test_AP_features)[:,0:64]\n",
    "\n",
    "test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "\n",
    "test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "test_y = np.asarray(test_df_LL)\n",
    "model.fit(input_data,train_y,nb_epoch=200,batch_size=128,callbacks=[\n",
    "       ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n",
    ",EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)])\n",
    "test_pred_y = model.predict([test_id, test_rssi_floor, test_building_id])\n",
    "test_pred_y = test_pred_y * ranges + bias \n",
    "error_analysis(test_y, test_pred_y)\n",
    "\"\"\"one hot encode the dummy_labels.\n",
    "this is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \n",
    "as the column names\n",
    "\"\"\"\n",
    "#train_X,train_y, val_X, val_y = train_val_split(train_AP_features,train_labels)\n",
    "\n",
    "#Turn the given validation set into a testing set\n",
    "#test_df = pd.read_csv(path_validation,header = 0)\n",
    "#test_AP_features = (np.asarray(test_df.loc[:,'WAP001':'WAP520']))/200+0.5\n",
    "#test_labels = np.asarray(test_df[\"BUILDINGID\"].map(str) + test_df[\"FLOOR\"].map(str))\n",
    "#test_labels = np.asarray(pd.get_dummies(test_labels))\n",
    "#input_size = 520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 3)\n",
      "[0 1 2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_8 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-dd4ef1168fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0minput_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0moutput_layer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_layer_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# the corresponding TF subgraph inside `backend.get_graph()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 611\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    173\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm_8 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 3]"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    #print(seq_in, '->', seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length))\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "input_layer = L.Input(shape=(3,))\n",
    "input_layer1 = L.Reshape((-1, 1))(input_layer)\n",
    "y = L.LSTM(32,return_sequences=False, activation='relu')(input_layer)\n",
    "output_layer_1 = L.Dense(1, name='output')(y)\n",
    "model = M.Model([input_layer], [output_layer_1])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
