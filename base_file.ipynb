{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhangzheng/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,sgd\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "#train_files,test_files = get_data_files()\n",
    "#print(train_files)\n",
    "def train_val_split(train_AP_features,train_labels,fp_ratio):\n",
    "    #generate len(train_AP_features) of floats in between 0 and 1\n",
    "    train_val_split = np.random.rand(len(train_AP_features))\n",
    "    #convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "    train_val_split = train_val_split < fp_ratio #should contain ~70% percent true\n",
    "    # We will then split our given training set into training + validation \n",
    "    train_X = train_AP_features[train_val_split]\n",
    "    train_y = train_labels[train_val_split]\n",
    "    val_X = train_AP_features[~train_val_split]\n",
    "    val_y = train_labels[~train_val_split]\n",
    "    return train_X,train_y, val_X, val_y\n",
    "\n",
    "def normalization(data):\n",
    "    minVals = data.min(0)\n",
    "    maxVals = data.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normData = (data - minVals)/ranges\n",
    "    return normData,ranges,minVals\n",
    "\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv(file_name,header = 0)\n",
    "    #print(df.head(2))\n",
    "    AP_strengths = df.loc[:,'WAP001':'WAP520']\n",
    "    AP_strengths = AP_strengths.replace([100], [-100])\n",
    "    print(AP_strengths.head(2))\n",
    "    df_xy = df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "    labels = np.asarray(df_xy)\n",
    "    AP_features = (np.asarray(AP_strengths))\n",
    "    \n",
    "    building_ids_str = df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "    building_floors_str = df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "    return AP_features, building_ids_str, building_floors_str, labels\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return math.sqrt((y_true[0]-y_pred[0])**2+(y_true[1]-y_pred[1])**2)\n",
    "\n",
    "\n",
    "def rms(list):\n",
    "    sum = 0\n",
    "    for term in list:\n",
    "        sum+= term*term\n",
    "    rms = math.sqrt(sum / len(list))\n",
    "    return rms\n",
    "def save_to_log(file_name,preds_pos):\n",
    "    write_file = open(file_name,'w')\n",
    "    for pos in preds_pos:\n",
    "        line = str(pos[0])+','+str(pos[1])+'\\n'\n",
    "        write_file.write(line)\n",
    "    return \n",
    "def load_log(file_name):\n",
    "    read_file = open(file_name,'r')\n",
    "    lines = read_file.readlines()\n",
    "    pred_pos = []\n",
    "    for line in lines:\n",
    "        pos = line.split(',')\n",
    "        x = float(pos[0])\n",
    "        y = float(pos[1])\n",
    "        pred_pos.append([x,y])\n",
    "    return pred_pos\n",
    "\n",
    "def cdf(error):\n",
    "    count = len(error)\n",
    "    cdf_y = [i/count for i in range(count)]\n",
    "    error_sorted = sorted(error)\n",
    "    plt.xlim(0,50)\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(error_sorted, cdf_y)\n",
    "    plt.show()\n",
    "    return cdf_y,error_sorted\n",
    "\n",
    "def error_analysis(pred_y,true_y):\n",
    "    error =np.sqrt((pred_y[:,0]-true_y[:,0])**2+(pred_y[:,1]-true_y[:,1])**2)\n",
    "\n",
    "    rms_error = rms(error)\n",
    "    print('rms_error:', rms_error)\n",
    "    mean_error = sum(error)/len(error)\n",
    "    print('mean_error:', mean_error)\n",
    "    print(\"generating cdf:\")\n",
    "    cdf_y,error_sorted = cdf(error)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input: \n",
    "def mlp_regression():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(512, input_dim=input_size, activation='relu', bias=True))\n",
    "    model1.add(Dense(512, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(128, activation='relu', bias=True))\n",
    "    model1.add(Dense(32, activation='relu', bias=True))\n",
    "    model1.add(Dense(2, activation='sigmoid', bias=True))\n",
    "    #model1.compile(optimizer='sgd', loss=[mean_squared_error_index],metrics=[mean_squared_error_index])\n",
    "    return model1    \n",
    "\n",
    "def lstm(input_data):\n",
    "    wapid_dim = input_data[0].shape[1]\n",
    "    wapid_input_layer = L.Input(shape=(wapid_dim,))\n",
    "    wap_emb = L.Embedding(520,1)(wapid_input_layer)\n",
    "    wap_emb = L.Flatten()(wap_emb)\n",
    "    \n",
    "    rssi_f_dim = input_data[1].shape[1]\n",
    "    rssi_f_input_layer = L.Input(shape=(rssi_f_dim,))\n",
    "    rssi_f = L.BatchNormalization()(rssi_f_input_layer)\n",
    "    rssi_f_feature = L.Dense(64, activation='relu')(rssi_f)\n",
    "    \n",
    "    \n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    site_emb = L.Embedding(13, 1)(input_site_layer)\n",
    "    site_emb = L.Flatten()(site_emb)\n",
    "    x = L.Concatenate(axis=1)([wap_emb, rssi_f_feature, site_emb])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    x = L.Reshape((-1, 1))(x)\n",
    "    print(x.shape)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.LSTM(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, activation='sigmoid')(x)\n",
    "    x = L.LSTM(16, dropout=0.1, return_sequences=False, activation='sigmoid')(x)\n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    model = M.Model([wapid_input_layer, rssi_f_input_layer, input_site_layer], [output_layer_1])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7541.2643  4.864921e+06      2           1      106   \n",
      "1     100  ...     100 -7536.6212  4.864934e+06      2           1      106   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 2       2       23  1371713733  \n",
      "1                 2       2       23  1371713691  \n",
      "\n",
      "[2 rows x 529 columns]\n",
      "floor id [[2]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n",
      "building id: [1 1 1 ... 1 1 1]\n",
      "[0.025 0.04  0.045 0.045 0.055 0.085 0.09  0.095 0.095 0.105 0.11  0.16\n",
      " 0.16  0.16  0.165 0.23  0.235 1.    1.    1.    1.    1.    1.    1.\n",
      " 1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.\n",
      " 1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.\n",
      " 1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.\n",
      " 1.    1.    1.    1.    2.   ] (19937, 65)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-770d0f151967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rssi_floor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_building_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#import tensorflow_addons as tfa\n",
    "#from tensorflow_addons.layers import WeightNormalization\n",
    "#from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "path_train = \"trainingData.csv\"\n",
    "path_validation = \"validationData.csv\"\n",
    "train_df = pd.read_csv(path_train,header = 0)\n",
    "print(train_df.head(2))\n",
    "\n",
    "train_AP_strengths =train_df.loc[:,'WAP001':'WAP520']\n",
    "train_AP_features = (np.asarray(train_AP_strengths))/200 + 0.5\n",
    "building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "floor_enc = LabelEncoder()\n",
    "floor_enc.fit(building_floors_str)\n",
    "floor_id = floor_enc.transform(building_floors_str)\n",
    "floor_id = floor_id.reshape(-1,1)\n",
    "print(\"floor id\",(floor_id))\n",
    "building_enc = LabelEncoder()\n",
    "building_enc.fit(building_ids_str)\n",
    "building_id = building_enc.transform(building_ids_str)\n",
    "train_building_id = building_id.reshape(-1,1)\n",
    "print(\"building id:\",(building_id))\n",
    "\n",
    "\n",
    "##### 1) RSSI_FLOOR 2) SSID 3) BUILDING_ID\n",
    "train_id = np.argsort(train_AP_features)[:,0:64]\n",
    "train_rssi = np.sort(train_AP_features)[:,0:64]\n",
    "\n",
    "train_rssi_floor = np.hstack((train_rssi,floor_id))\n",
    "print(train_rssi_floor[0], train_rssi_floor.shape)\n",
    "#print(train_id[0],train_rssi[0],train_AP_features[0])\n",
    "\n",
    "input_data = [train_id, train_rssi_floor, train_building_id]\n",
    "model = lstm(input_data)\n",
    "model.summary()\n",
    "\n",
    "train_df_LL = train_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "train_labels = np.asarray(train_df_LL)\n",
    "train_y,ranges,bias =  normalization(train_labels)\n",
    "model.fit(input_data,train_y,nb_epoch=500,batch_size=128)\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "print(test_df.head(2))\n",
    "test_AP_strengths =test_df.loc[:,'WAP001':'WAP520']\n",
    "test_AP_features = (np.asarray(test_AP_strengths))/200 + 0.5\n",
    "test_building_ids_str = test_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "test_building_floors_str = test_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "#print(id_label)\n",
    "test_floor_enc = LabelEncoder()\n",
    "test_floor_enc.fit(building_floors_str)\n",
    "test_floor_id = test_floor_enc.transform(test_building_floors_str)\n",
    "test_floor_id = test_floor_id.reshape(-1,1)\n",
    "print(\"test floor id\",(test_floor_id))\n",
    "test_building_enc = LabelEncoder()\n",
    "test_building_enc.fit(test_building_ids_str)\n",
    "test_building_id = test_building_enc.transform(building_ids_str)\n",
    "test_building_id = test_building_id.reshape(-1,1)\n",
    "print(\"test building id:\",(test_building_id))\n",
    "test_id = np.argsort(test_AP_features)[:,0:64]\n",
    "test_rssi = np.sort(test_AP_features)[:,0:64]\n",
    "\n",
    "test_rssi_floor = np.hstack((test_rssi,test_floor_id))\n",
    "print(test_rssi_floor[0], test_rssi_floor.shape)\n",
    "\n",
    "test_df_LL = test_df.loc[:,'LONGITUDE':'LATITUDE']\n",
    "test_y = np.asarray(test_df_LL)\n",
    "model.fit(input_data,train_y,nb_epoch=200,batch_size=128,callbacks=[\n",
    "       ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n",
    ",EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)])\n",
    "test_pred_y = model.predict([test_id, test_rssi_floor, test_building_id])\n",
    "test_pred_y = test_pred_y * ranges + bias \n",
    "error_analysis(test_y, test_pred_y)\n",
    "\"\"\"one hot encode the dummy_labels.\n",
    "this is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \n",
    "as the column names\n",
    "\"\"\"\n",
    "#train_X,train_y, val_X, val_y = train_val_split(train_AP_features,train_labels)\n",
    "\n",
    "#Turn the given validation set into a testing set\n",
    "#test_df = pd.read_csv(path_validation,header = 0)\n",
    "#test_AP_features = (np.asarray(test_df.loc[:,'WAP001':'WAP520']))/200+0.5\n",
    "#test_labels = np.asarray(test_df[\"BUILDINGID\"].map(str) + test_df[\"FLOOR\"].map(str))\n",
    "#test_labels = np.asarray(pd.get_dummies(test_labels))\n",
    "#input_size = 520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 3)\n",
      "[0 1 2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_8 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-dd4ef1168fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0minput_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0moutput_layer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_layer_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# the corresponding TF subgraph inside `backend.get_graph()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 611\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    173\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm_8 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 3]"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    #print(seq_in, '->', seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length))\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "input_layer = L.Input(shape=(3,))\n",
    "input_layer1 = L.Reshape((-1, 1))(input_layer)\n",
    "y = L.LSTM(32,return_sequences=False, activation='relu')(input_layer)\n",
    "output_layer_1 = L.Dense(1, name='output')(y)\n",
    "model = M.Model([input_layer], [output_layer_1])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
